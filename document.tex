\subsection{Retrieval-Augmented Generation (RAG)}

Để tạo ra nội dung vừa chính xác vừa phù hợp ngữ cảnh, đồ án áp dụng kỹ thuật \textbf{Retrieval-Augmented Generation (RAG)}. Cốt lõi của kỹ thuật này là một pipeline hai giai đoạn: \textbf{Truy xuất (Retrieval)} và \textbf{Tái xếp hạng (Reranking)}. Giai đoạn truy xuất sử dụng phương pháp \textbf{Hybrid Search}, kết hợp ưu điểm của \textit{semantic search} để nắm bắt ý định và \textit{lexical search} với \textbf{BM25} để đảm bảo độ chính xác thuật ngữ. Quy trình này nhằm tối ưu đồng thời \textit{recall} (độ phủ) và \textit{precision} (độ chính xác).

\subsubsection{Kiến trúc Truy xuất và Tái xếp hạng}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Hinhve/hybrid_search.png}
    \caption{Sơ đồ kiến trúc Hybrid Search hai giai đoạn: Retrieval và Reranking.}
    \label{fig:hybrid_search}
\end{figure}

Kiến trúc hệ thống bao gồm hai giai đoạn chính:

\paragraph{Giai đoạn 1: Retrieval}
Giai đoạn này tập trung tối đa hóa \textit{recall} bằng cách thực hiện đồng thời hai phương pháp truy xuất:
\begin{itemize}
    \item \textbf{Dense Retrieval}: Sử dụng mô hình Bi-Encoder \texttt{AITeamVN/Vietnamese\_Embedding\_v2} để tạo các \textit{dense vector}, qua đó nắm bắt sự tương đồng ngữ nghĩa.
    \item \textbf{Sparse Retrieval}: Sử dụng thuật toán \textbf{BM25} (triển khai bởi \texttt{Qdrant/bm25}) để tạo các \textit{sparse vector} dựa trên tần suất từ khóa.
\end{itemize}
Điểm số từ hai phương pháp được chuẩn hóa Min-Max và kết hợp bằng kỹ thuật \textbf{Weighted Score Fusion}:
\[ S_{final}(d) = w_{dense} \cdot S_{dense\_norm}(d) + w_{sparse} \cdot S_{sparse\_norm}(d) \]
Trong đó, các trọng số được chọn là \(w_{dense}=0.7\) và \(w_{sparse}=0.3\). Đây là một tỷ lệ heuristic phổ biến, mang lại sự cân bằng hiệu quả giữa việc nắm bắt ngữ nghĩa và khớp từ khóa chính xác.

\paragraph{Giai đoạn 2: Reranking}
Giai đoạn này tập trung tối đa hóa \textit{precision}. Các ứng viên có điểm cao nhất từ giai đoạn 1 được một mô hình \textbf{Cross-Encoder} (\texttt{AITeamVN/Vietnamese\_Reranker}) đánh giá lại. Khác với Bi-Encoder, Cross-Encoder xử lý đồng thời cặp (truy vấn, tài liệu), cho phép cơ chế \textit{attention} thực hiện so sánh chéo sâu sắc, mang lại điểm số liên quan với độ chính xác vượt trội. Do chi phí tính toán cao, phương pháp này chỉ phù hợp để xếp hạng lại một tập hợp ứng viên nhỏ đã được sàng lọc.

\subsubsection{Lựa chọn Mô hình}

\paragraph{Mô hình Retrieval và Reranking cho Tiếng Việt}
Việc lựa chọn mô hình được thực hiện dựa trên các tiêu chí về hiệu năng, khả năng xử lý ngữ cảnh và sự tối ưu hóa cho ngôn ngữ tiếng Việt.
\begin{itemize}
    \item \textbf{Đối với Dense Retrieval (Bi-Encoder)}: Đồ án sử dụng \texttt{AITeamVN/Vietnamese\_Embedding\_v2}. Lý do chính cho sự lựa chọn này là khả năng xử lý văn bản dài với \textbf{độ dài ngữ cảnh tối đa là 2048 token}. Điều này cho phép mô hình mã hóa các đoạn tài liệu dài mà không bị cắt bớt, bảo toàn được ngữ nghĩa trọn vẹn. Hơn nữa, mô hình này được huấn luyện chuyên biệt trên một tập dữ liệu lớn gồm 1.1 triệu cặp văn bản tiếng Việt, đảm bảo hiệu suất truy xuất cao cho ngôn ngữ của đồ án.
    \item \textbf{Đối với Reranking (Cross-Encoder)}: Đồ án sử dụng \texttt{AITeamVN/Vietnamese\_Reranker}. Mô hình này được chọn vì hiệu suất vượt trội trong các tác vụ xếp hạng. Theo kết quả benchmark trên V-MTEB, \texttt{Vietnamese\_Reranker} đạt độ chính xác (Accuracy@1) và chỉ số MRR@10 cao nhất so với các mô hình embedding khác, bao gồm cả \texttt{Vietnamese\_Embedding\_v2} \cite{vmteb_github}. Kiến trúc Cross-Encoder vốn có độ chính xác cao hơn Bi-Encoder, và việc mô hình này cũng được tối ưu cho tiếng Việt khiến nó trở thành lựa chọn lý tưởng cho giai đoạn tái xếp hạng, nơi độ chính xác là ưu tiên hàng đầu.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Hinhve/embedding_benchmark.png}
    \caption{Kết quả benchmark của các mô hình embedding trên bộ dữ liệu V-MTEB \cite{benchmark_vnmteb}.}
    \label{fig:embedding_benchmark}
\end{figure}

\paragraph{Mô hình Generative}
Đồ án áp dụng một chiến lược kết hợp khi lựa chọn mô hình ngôn ngữ lớn (LLM) để tối ưu giữa chi phí và hiệu suất:
\begin{itemize}
    \item \textbf{Google Gemini (\texttt{gemini-1.5-flash})}: Dùng cho các tác vụ nền (background tasks) như xử lý email, nhờ chi phí tối ưu và khả năng xử lý đa phương thức.
    \item \textbf{DeepSeek}: Dùng cho tính năng tìm kiếm thông minh (Intelligent Search) yêu cầu phản hồi thời gian thực, nhờ hiệu suất API cao và thông lượng (throughput) tốt.
\end{itemize}
Sự kết hợp này cân bằng giữa hiệu năng cho các tác vụ tương tác thời gian thực và chi phí cho các tác vụ nền. 