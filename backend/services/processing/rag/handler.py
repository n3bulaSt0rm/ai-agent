import os
import base64
import json
import asyncio
import logging
import time
import google.generativeai as genai
import functools

from datetime import datetime, time as datetime_time
from typing import Dict, Any, List, Optional, Tuple
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.utils import parsedate_to_datetime
from pathlib import Path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from backend.services.processing.rag.retrievers.qdrant_retriever import VietnameseQueryModule, create_query_module
from backend.services.processing.rag.embedders.text_embedder import VietnameseEmbeddingModule
from backend.core.config import settings
from backend.db.metadata import get_metadata_db

from backend.services.processing.rag.draft_monitor import EmailDraftMonitor

from backend.services.processing.rag.utils import (
    create_deepseek_client, DeepSeekAPIClient, 
    extract_image_attachments, extract_text_content, extract_all_attachments,
    call_deepseek_async
)

from backend.services.processing.rag.extractors.gemini.gemini_email_processor import GeminiEmailProcessor
from backend.services.processing.rag.gmail_api_monitor import create_gmail_api_monitor
from backend.services.processing.rag.gmail_indexing_worker import GmailIndexingWorker
from backend.services.processing.rag.gmail_cleanup_worker import GmailCleanupWorker

logger = logging.getLogger(__name__)

class GmailHandler:
    """
    Gmail handler that processes emails with multimodal content using Gemini
    """
    
    def __init__(self, token_path: str = None):
        """
        Initialize Gmail handler with Gemini always enabled.
        
        Args:
            token_path: Path to the token JSON file  
        """
        self.token_path = token_path or settings.GMAIL_TOKEN_PATH
        self.service = None
        self.user_id = 'me'
        
        try:
            if settings.GOOGLE_API_KEY:
                genai.configure(api_key=settings.GOOGLE_API_KEY)
                logger.debug("Gemini API configured")
            
            self.gemini_processor = GeminiEmailProcessor()
            logger.debug("Gemini email processor initialized")
        except Exception as e:
            logger.error(f"Gemini initialization failed: {e}")
            raise Exception(f"Required Gemini processor failed to initialize: {e}")
        
        self.deepseek_api_key = settings.DEEPSEEK_API_KEY
        self.deepseek_api_url = settings.DEEPSEEK_API_URL
        self.deepseek_model = settings.DEEPSEEK_MODEL
        
        self.deepseek_client = create_deepseek_client(
            deepseek_api_key=self.deepseek_api_key,
            deepseek_api_url=self.deepseek_api_url,
            deepseek_model=self.deepseek_model
        )
        
        self.query_module = None
        
        self.metadata_db = get_metadata_db()
        
        self.draft_monitor = None
        self.api_monitor = None
        
        self.background_worker = None
        self.cleanup_worker = None
        
        if not self.deepseek_api_key:
            logger.warning("DEEPSEEK_API_KEY not set in settings")
    
    def _initialize_managers(self):
        """Initialize draft monitor and API monitor after authentication."""
        if not self.service:
            logger.error("Gmail service not authenticated, cannot initialize managers")
            return
        
        self.draft_monitor = EmailDraftMonitor(
            service=self.service,
            metadata_db=self.metadata_db,
            user_id=self.user_id
        )
        
        self.api_monitor = create_gmail_api_monitor(gmail_handler=self, poll_interval=settings.GMAIL_POLL_INTERVAL)
        
        logger.info("Draft monitor and Gmail API monitor initialized successfully")

    def _init_indexing_worker(self):
        """Initialize indexing worker (called separately after authentication)"""
        if not self.service:
            logger.error("Gmail service not authenticated, cannot initialize indexing worker")
            return False
        
        try:
            if self.query_module is None:
                self._init_query_module()
            
            embedding_module = None
            if hasattr(self.query_module, 'embedding_module'):
                embedding_module = self.query_module.embedding_module
                logger.info("Using shared embedding module from query module")
            
            self.background_worker = GmailIndexingWorker(
                gmail_service=self.service,
                user_id=self.user_id,
                gemini_processor=self.gemini_processor,
                embedding_module=embedding_module
            )
            self.background_worker.start()
            logger.info("Gmail indexing worker initialized and started")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize indexing worker: {e}")
            return False

    def _init_cleanup_worker(self):
        """Initialize cleanup worker for outdated threads"""
        try:
            # Initialize query module first to get embedding module
            if self.query_module is None:
                self._init_query_module()
            
            # Get embedding module from query module
            embedding_module = None
            if hasattr(self.query_module, 'embedding_module'):
                embedding_module = self.query_module.embedding_module
                logger.info("Using shared embedding module from query module")
            
            self.cleanup_worker = GmailCleanupWorker(embedding_module=embedding_module)
            self.cleanup_worker.start()
            logger.info("Gmail cleanup worker initialized and started")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize cleanup worker: {e}")
            return False

    def _init_query_module(self):
        """
        Initialize the Vietnamese Query Module if not already initialized
        """
        if self.query_module is not None:
            return
            
        try:
            # Try to get embedding module and memory manager from server modules
            memory_manager = None
            embedding_module = None
            try:
                # Import here to avoid circular imports
                from backend.services.processing.server import modules
                if hasattr(modules, 'cuda_memory_manager') and modules.cuda_memory_manager:
                    memory_manager = modules.cuda_memory_manager
                    logger.debug("Using shared CUDA Memory Manager from server")
                if hasattr(modules, 'embedding_module') and modules.embedding_module:
                    embedding_module = modules.embedding_module
                    logger.debug("Using shared Embedding Module from server")
            except (ImportError, AttributeError) as e:
                logger.warning(f"Could not import modules from server: {e}")
            
            # Create embedding module only if not available from server
            if not embedding_module:
                logger.info("Creating new VietnameseEmbeddingModule for Gmail handler")
                embedding_module = VietnameseEmbeddingModule(
                    qdrant_host=settings.QDRANT_HOST,
                    qdrant_port=settings.QDRANT_PORT,
                    collection_name=settings.QDRANT_COLLECTION_NAME,
                    dense_model_name=settings.DENSE_MODEL_NAME,
                    sparse_model_name=settings.SPARSE_MODEL_NAME,
                    reranker_model_name=settings.RERANKER_MODEL_NAME,
                    vector_size=settings.VECTOR_SIZE,
                    memory_manager=memory_manager 
                )
            
            self.query_module = create_query_module(
                embedding_module=embedding_module,
                deepseek_api_key=self.deepseek_api_key,
                memory_manager=memory_manager,  
                deepseek_model=self.deepseek_model,
                limit=5,
                candidates_limit=10,
                dense_weight=0.8,
                sparse_weight=0.2,
                normalization="min_max",
                candidates_multiplier=3
            )
            
            logger.debug(f"Vietnamese Query Module initialized with hybrid search and reranking")
            
        except Exception as e:
            logger.error(f"Error initializing Vietnamese Query Module: {e}")
            raise

    def authenticate(self) -> None:
        creds = None
        
        # Check if token file exists and load it directly
        if os.path.exists(self.token_path):
            try:
                # Load token data
                token_data = json.load(open(self.token_path))
                
                # Create credentials using the token data
                creds = Credentials.from_authorized_user_info(token_data)
                
                # If token is expired but has refresh token, refresh it
                if creds.expired and creds.refresh_token:
                    creds.refresh(Request())
                    # Save the refreshed token
                    with open(self.token_path, 'w') as token:
                        token.write(creds.to_json())
                    logger.info(f"Refreshed and saved authentication token to {self.token_path}")
            except Exception as e:
                logger.error(f"Error loading or refreshing token file: {e}")
                raise
        else:
            logger.error(f"Token file not found at {self.token_path}")
            raise FileNotFoundError(f"Token file not found: {self.token_path}")
                
        # Build Gmail service
        try:
            self.service = build('gmail', 'v1', credentials=creds)
            logger.info("Successfully authenticated with Gmail API")
            
            # Initialize managers after authentication
            self._initialize_managers()
        except Exception as e:
            logger.error(f"Error building Gmail service: {e}")
            raise
            
    async def process_unread_email(self) -> List[Dict[str, Any]]:
        """
        Process unread emails using new optimized logic with Gemini conversation.
        
        Returns:
            List of processed email thread information
            
        Raises:
            Exception: If processing emails fails
        """
        if not self.service:
            try:
                self.authenticate()
            except Exception as e:
                logger.error(f"Authentication failed: {e}")
                return []
                
        try:
            # Get unread messages
            results = self.service.users().messages().list(
                userId=self.user_id, 
                q="is:unread").execute()
                
            messages = results.get('messages', [])
            
            if not messages:
                logger.debug("No unread messages found")
                return []
            
            # Group messages by thread_id
            thread_groups = {}
            for message in messages:
                msg = self.service.users().messages().get(
                    userId=self.user_id, 
                    id=message['id']).execute()
                    
                thread_id = msg['threadId']
                if thread_id not in thread_groups:
                    thread_groups[thread_id] = []
                thread_groups[thread_id].append(msg)
            
            logger.info(f"Found {len(messages)} unread emails in {len(thread_groups)} threads")
            
            processed_results = []
            
            # Process each thread
            for thread_id, thread_messages in thread_groups.items():
                try:
                    result = await self._process_thread(thread_id, thread_messages)
                    if result:
                        processed_results.append(result)
                except Exception as e:
                    logger.error(f"Error processing thread {thread_id}: {e}")
                    continue
            
            return processed_results
            
        except HttpError as e:
            logger.error(f"Error fetching emails: {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            return []
            
    def _get_email_body(self, message: Dict) -> str:
        """
        Extract email body from Gmail message, including processing images with Gemini
        
        Args:
            message: Gmail message object
            
        Returns:
            Processed email body content
        """
        try:
            message_id = message.get('id')
            payload = message.get('payload', {})
            
            email_text = extract_text_content(payload)
            
            image_attachments = extract_image_attachments(self.service, self.user_id, payload, message_id)
            
            # Process attachments with Gemini if present
            if image_attachments:
                try:
                    logger.info(f"Processing {len(image_attachments)} images with Gemini")
                    return self.gemini_processor.process_email_with_attachments(
                        email_text=email_text, 
                        image_attachments=image_attachments,
                        pdf_attachments=[]
                    )
                except Exception as e:
                    logger.error(f"Gemini error: {e}")
            
            # Add image info if present
            if image_attachments:
                image_info = f"\n\n=== ·∫¢NH ƒê√çNH K√àM ===\n"
                for i, img in enumerate(image_attachments, 1):
                    image_info += f"üì∑ ·∫¢nh {i}: {img.get('filename', f'image_{i}')}\n"
                return email_text + image_info
            
            return email_text
            
        except Exception as e:
            logger.error(f"Error extracting email body: {e}")
            return "[L·ªói tr√≠ch xu·∫•t n·ªôi dung email]"

    async def _process_thread(self, thread_id: str, thread_messages: List[Dict]) -> Optional[Dict[str, Any]]:
        try:
            logger.info(f"Processing thread {thread_id} with {len(thread_messages)} messages")
            
            existing_draft_id = self.draft_monitor.check_existing_draft(thread_id)
            if existing_draft_id:
                logger.info(f"Found existing draft {existing_draft_id}, deleting")
                self.draft_monitor.delete_draft(existing_draft_id)
            
            thread_info = self.metadata_db.get_gmail_thread_info(thread_id)
            last_processed_message_id = thread_info.get('last_processed_message_id') if thread_info else None
            
            all_thread_emails = await self._fetch_thread_emails_with_attachments(
                thread_id, last_processed_message_id
            )
            
            if not all_thread_emails:
                logger.warning(f"No emails to process for thread {thread_id}")
                return None
            
            conversation = await self._create_gemini_conversation_for_thread(all_thread_emails)
            if not conversation:
                logger.error(f"Failed to create Gemini conversation for thread {thread_id}")
                return None
            
            questions, context_summary = await self._extract_questions_with_gemini(conversation, all_thread_emails)
            
            if not questions:
                logger.info(f"No questions found in thread {thread_id}")
                return {"thread_id": thread_id, "status": "no_questions"}
            
            if self.query_module is None:
                self._init_query_module()
            
            summarized_results = []
            for i in range(0, len(questions), 2):
                group = questions[i:i+2]
                logger.debug(f"Processing question group {i//2 + 1}: {len(group)} questions")
                
                group_info = ""
                group_qa_info = ""  # Separate info for EMAIL_QA results
                group_queries = []
                
                for j, question in enumerate(group):
                    group_queries.append(question)
                    
                    # Search in both collections using optimized method
                    search_results, qa_results = self._search_multiple_collections(question)
                    
                    # Format main collection results
                    if search_results:
                        group_info += f"C√¢u h·ªèi {j+1}: {question}\n"
                        for k, result_item in enumerate(search_results):
                            content = result_item.get("content", "") if isinstance(result_item, dict) else str(result_item)
                            metadata = result_item.get("metadata", {}) if isinstance(result_item, dict) else {}
                            file_created_at = metadata.get("file_created_at")
                            source = metadata.get("source")
                            
                            group_info += f"T√†i li·ªáu {k+1}:"
                            if file_created_at:
                                group_info += f" (C·∫≠p nh·∫≠t: {file_created_at})"
                            if source and not source.startswith("gmail_thread"):
                                group_info += f" [Ngu·ªìn: {source}]"
                            group_info += f"\n{content}\n\n"
                    else:
                        group_info += f"C√¢u h·ªèi {j+1}: {question}\nKh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan.\n\n"
                    
                    # Format EMAIL_QA collection results (without source citation requirement)
                    if qa_results:
                        group_qa_info += f"C√¢u h·ªèi {j+1}: {question}\n"
                        for k, qa_item in enumerate(qa_results):
                            qa_content = qa_item.get("content", "") if isinstance(qa_item, dict) else str(qa_item)
                            qa_metadata = qa_item.get("metadata", {}) if isinstance(qa_item, dict) else {}
                            qa_file_created_at = qa_metadata.get("file_created_at")
                            
                            group_qa_info += f"Q&A {k+1}:"
                            if qa_file_created_at:
                                group_qa_info += f" (C·∫≠p nh·∫≠t: {qa_file_created_at})"
                            group_qa_info += f"\n{qa_content}\n\n"
                
                # Create combined summarization prompt
                summarization_prompt = f"""
                H√£y t√≥m t·∫Øt l·∫°i c√°c n·ªôi dung li√™n quan ƒë·∫øn c√°c c√¢u h·ªèi sau m·ªôt c√°ch ch√≠nh x√°c, ƒë·∫ßy ƒë·ªß th√¥ng tin, s√∫c t√≠ch:
                
                C√°c c√¢u h·ªèi: {', '.join(group_queries)}
                
                Th√¥ng tin t·ª´ t√†i li·ªáu ch√≠nh th·ª©c:
                {group_info}
                
                Th√¥ng tin t·ª´ Q&A tr∆∞·ªõc ƒë√¢y:
                {group_qa_info}
                
                L∆ØU √ù QUAN TR·ªåNG:
                - ∆Øu ti√™n th√¥ng tin c√≥ ng√†y c·∫≠p nh·∫≠t g·∫ßn ƒë√¢y nh·∫•t t·ª´ c·∫£ hai ngu·ªìn (t√†i li·ªáu ch√≠nh th·ª©c v√† Q&A).
                - N·∫øu c√≥ nhi·ªÅu th√¥ng tin v·ªÅ c√πng m·ªôt ch·ªß ƒë·ªÅ v·ªõi c√°c ng√†y c·∫≠p nh·∫≠t kh√°c nhau, ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ngu·ªìn c√≥ ng√†y c·∫≠p nh·∫≠t m·ªõi nh·∫•t.
                - Khi c√≥ th√¥ng tin ngu·ªìn t·ª´ t√†i li·ªáu ch√≠nh th·ª©c, h√£y ghi r√µ ngu·ªìn trong t√≥m t·∫Øt ƒë·ªÉ c√≥ th·ªÉ tr√≠ch d·∫´n sau n√†y.
                - Th√¥ng tin t·ª´ Q&A tr∆∞·ªõc ƒë√¢y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ng kh√¥ng c·∫ßn tr√≠ch d·∫´n ngu·ªìn c·ª• th·ªÉ.
                - ƒê·ªëi v·ªõi th√¥ng tin kh√¥ng c√≥ ng√†y c·∫≠p nh·∫≠t r√µ r√†ng, coi nh∆∞ c≈© h∆°n so v·ªõi th√¥ng tin c√≥ ng√†y c·∫≠p nh·∫≠t.
                - Khi so s√°nh th√¥ng tin t·ª´ t√†i li·ªáu ch√≠nh th·ª©c v√† Q&A c√≥ c√πng ng√†y c·∫≠p nh·∫≠t, ∆∞u ti√™n th√¥ng tin t·ª´ t√†i li·ªáu ch√≠nh th·ª©c.
                - Lu√¥n ghi r√µ ng√†y c·∫≠p nh·∫≠t th√¥ng tin trong t√≥m t·∫Øt khi c√≥ (v√≠ d·ª•: "Theo th√¥ng tin c·∫≠p nh·∫≠t ng√†y 15/03/2024...").
                """
                
                try:
                    summary_response = await self._ask_gemini(conversation, summarization_prompt)
                    summarized_results.append({
                        "queries": group_queries,
                        "summary": summary_response
                    })
                except Exception as e:
                    logger.error(f"Error summarizing group {group_queries}: {e}")
                    summarized_results.append({
                        "queries": group_queries,
                        "summary": f"L·ªói x·ª≠ l√Ω th√¥ng tin cho c√°c c√¢u h·ªèi: {', '.join(group_queries)}"
                    })
            
            email_response = await self._generate_email_response_with_gemini(
                conversation, all_thread_emails, summarized_results
            )
            
            conversation = None
            
            newest_email = thread_messages[-1]
            headers = {header['name']: header['value'] 
                      for header in newest_email['payload']['headers']}
            to_address = headers.get('From', '')
            subject = headers.get('Subject', '')
            
            draft_id = await self.create_draft_email(
                to=to_address,
                subject=subject,
                body=email_response,
                thread_id=thread_id
            )
            
            if draft_id:
                newest_message_id = newest_email['id']
                self.metadata_db.upsert_gmail_thread(
                    thread_id=thread_id,
                    context_summary=context_summary,
                    current_draft_id=draft_id,
                    last_processed_message_id=newest_message_id
                )
                
                marked_count = 0
                for msg in thread_messages:
                    try:
                        self.mark_as_read(msg['id'])
                        marked_count += 1
                    except Exception as mark_error:
                        logger.error(f"Failed to mark message {msg['id']} as read: {mark_error}")
                
                logger.info(f"Successfully processed thread {thread_id}, draft ID: {draft_id}, marked {marked_count}/{len(thread_messages)} messages as read")
                
                return {
                    "thread_id": thread_id,
                    "draft_id": draft_id,
                    "processed_messages": marked_count,
                    "context_summary": context_summary,
                    "status": "success"
                }
            else:
                logger.error(f"Failed to create draft for thread {thread_id}")
                return None
                
        except Exception as e:
            logger.error(f"Error in _process_thread for thread {thread_id}: {e}")
            return None

    
    def mark_as_read(self, message_id: str) -> None:
        """
        Mark an email as read.
        
        Args:
            message_id: Gmail message ID
            
        Raises:
            Exception: If marking as read fails
        """
        try:
            self.service.users().messages().modify(
                userId=self.user_id,
                id=message_id,
                body={'removeLabelIds': ['UNREAD']}
            ).execute()
            logger.info(f"Marked message {message_id} as read")
        except Exception as e:
            logger.error(f"Error marking message as read: {e}")
            raise
            

    async def _fetch_thread_emails_with_attachments(self, thread_id: str, last_processed_message_id: str = None) -> List[Dict[str, Any]]:
        try:
            thread_messages = self.service.users().threads().get(
                userId=self.user_id, 
                id=thread_id,
                format='full'
            ).execute()
            
            messages = thread_messages.get('messages', [])
            
            if not messages:
                return []
            
            filtered_messages = []
            if last_processed_message_id:
                found_last = False
                for message in messages:
                    if message['id'] == last_processed_message_id:
                        found_last = True
                        continue
                    if found_last:
                        filtered_messages.append(message)
                
                if not found_last:
                    logger.warning(f"Last processed message {last_processed_message_id} not found, processing all messages")
                    filtered_messages = messages
            else:
                filtered_messages = messages
            
            if not filtered_messages:
                logger.info(f"No new messages to process for thread {thread_id}")
                return []
            
            processed_emails = []
            for message in filtered_messages:
                try:
                    headers = {h['name']: h['value'] for h in message['payload']['headers']}
                    
                    # Extract text content
                    email_text = extract_text_content(message['payload'])
                    
                    # Extract all attachments (images and PDFs)
                    attachments = extract_all_attachments(
                        self.service, self.user_id, message['payload'], message['id']
                    )
                    
                    # Process with Gemini if attachments exist
                    if attachments:
                        image_attachments = [att for att in attachments if att.get('attachment_type') == 'image']
                        pdf_attachments = [att for att in attachments if att.get('attachment_type') == 'pdf']
                        
                        try:
                            processed_content = self.gemini_processor.process_email_with_attachments(
                                email_text=email_text, 
                                image_attachments=image_attachments,
                                pdf_attachments=pdf_attachments
                            )
                        except Exception as e:
                            logger.error(f"Gemini processing failed for message {message['id']}: {e}")
                            processed_content = email_text + f"\n--- L·ªói x·ª≠ l√Ω ƒë√≠nh k√®m: {str(e)} ---"
                    else:
                        processed_content = email_text
                    
                    processed_emails.append({
                        'id': message['id'],
                        'from': headers.get('From', ''),
                        'to': headers.get('To', ''),
                        'subject': headers.get('Subject', ''),
                        'date': headers.get('Date', ''),
                        'content': processed_content,
                        'original_text': email_text,
                        'has_attachments': len(attachments) > 0,
                        'attachment_count': len(attachments)
                    })
                    
                except Exception as e:
                    logger.error(f"Error processing message {message['id']}: {e}")
                    continue
            
            logger.info(f"Processed {len(processed_emails)} emails from thread {thread_id}")
            return processed_emails
            
        except Exception as e:
            logger.error(f"Error fetching thread emails for {thread_id}: {e}")
            return []

    async def _create_gemini_conversation_for_thread(self, thread_emails: List[Dict[str, Any]]) -> Optional[Any]:
        try:
            import google.generativeai as genai
            
            gmail_address = settings.GMAIL_EMAIL_ADDRESS 
            system_message = f"""
B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp h·ªó tr·ª£ {gmail_address} trong vi·ªác ph√¢n t√≠ch v√† x·ª≠ l√Ω email t·ª´ sinh vi√™n.

TH√îNG TIN QUAN TR·ªåNG:
- B·∫°n ƒëang h·ªó tr·ª£ t√†i kho·∫£n: {gmail_address}
- Vai tr√≤: Tr·ª£ l√Ω ph√≤ng c√¥ng t√°c sinh vi√™n
- Nhi·ªám v·ª•: Ph√¢n t√≠ch email, t√≥m t·∫Øt n·ªôi dung, tr√≠ch xu·∫•t c√¢u h·ªèi v√† t·∫°o ph·∫£n h·ªìi chuy√™n nghi·ªáp

NGUY√äN T·∫ÆC HO·∫†T ƒê·ªòNG:
1. Ph√¢n t√≠ch to√†n b·ªô thread email ƒë·ªÉ hi·ªÉu context ƒë·∫ßy ƒë·ªß
2. T√≥m t·∫Øt n·ªôi dung m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c
3. Tr√≠ch xu·∫•t c√°c c√¢u h·ªèi ch∆∞a ƒë∆∞·ª£c gi·∫£i ƒë√°p t·ª´ sinh vi√™n
4. T·∫°o ph·∫£n h·ªìi chuy√™n nghi·ªáp, th√¢n thi·ªán nh∆∞ng trang tr·ªçng
5. ƒê·∫£m b·∫£o th√¥ng tin ch√≠nh x√°c v√† c·∫≠p nh·∫≠t

H√£y s·∫µn s√†ng ph√¢n t√≠ch thread email.
"""
            
            model = genai.GenerativeModel("gemini-2.0-flash")
            chat = model.start_chat(history=[])
            
            # Send system message
            response = chat.send_message(system_message)
            
            logger.info("Successfully created Gemini conversation for thread analysis")
            return chat
            
        except Exception as e:
            logger.error(f"Error creating Gemini conversation: {e}")
            return None

    async def _extract_questions_with_gemini(self, conversation: Any, thread_emails: List[Dict[str, Any]]) -> tuple[List[str], str]:
        """
        Extract questions and create context summary using Gemini.
        
        Args:
            conversation: Gemini conversation object
            thread_emails: List of thread email data
            
        Returns:
            Tuple of (questions_list, context_summary)
        """
        try:
            # Prepare thread content for analysis
            thread_content = ""
            for i, email in enumerate(thread_emails, 1):
                thread_content += f"""
=== EMAIL {i} ===
T·ª´: {email['from']}
ƒê·∫øn: {email['to']}
Ti√™u ƒë·ªÅ: {email['subject']}
Ng√†y: {email['date']}
N·ªôi dung:
{email['content']}

"""
            
            analysis_prompt = f"""
H√£y ph√¢n t√≠ch thread email sau v√† th·ª±c hi·ªán 2 nhi·ªám v·ª•:

1. T√ìM T·∫ÆT CONTEXT: T·∫°o t√≥m t·∫Øt chi ti·∫øt, ƒë·∫ßy ƒë·ªß th√¥ng tin v·ªÅ to√†n b·ªô thread email (ƒë·ªëi v·ªõi n·ªôi dung t·ª´ attachment th√¨ c·∫ßn ng·∫Øn g·ªçn, s√∫c t√≠ch nh∆∞ng v·∫´n ƒë·∫ßy ƒë·ªß th√¥ng tin b·ªï tr·ª£ cho email c·ªßa ng∆∞·ªùi h·ªèi)

2. TR√çCH XU·∫§T C√ÇU H·ªéI: T√¨m t·∫•t c·∫£ c√°c c√¢u h·ªèi/y√™u c·∫ßu th√¥ng tin t·ª´ sinh vi√™n m√† ch∆∞a ƒë∆∞·ª£c gi·∫£i ƒë√°p ho·∫∑c c·∫ßn th√¥ng tin th√™m

THREAD EMAIL:
{thread_content}

L∆ØU √ù QUAN TR·ªåNG:
- Ch·ªâ tr√≠ch xu·∫•t c√¢u h·ªèi t·ª´ email sinh vi√™n (kh√¥ng ph·∫£i t·ª´ email ph·∫£n h·ªìi c·ªßa ph√≤ng c√¥ng t√°c sinh vi√™n)
- C√¢u h·ªèi ph·∫£i r√µ r√†ng v√† c·∫ßn th√¥ng tin c·ª• th·ªÉ
- B·ªè qua c√°c l·ªùi ch√†o h·ªèi, c·∫£m ∆°n ƒë∆°n thu·∫ßn
- M·ªói c√¢u h·ªèi ph·∫£i ho√†n ch·ªânh v√† c√≥ th·ªÉ t√¨m ki·∫øm ƒë∆∞·ª£c

Tr·∫£ v·ªÅ JSON v·ªõi format:
{{
    "context_summary": "T√≥m t·∫Øt chi ti·∫øt to√†n b·ªô thread email...",
    "questions": [
        "C√¢u h·ªèi 1 ƒë∆∞·ª£c vi·∫øt r√µ r√†ng v√† ho√†n ch·ªânh",
        "C√¢u h·ªèi 2 ƒë∆∞·ª£c vi·∫øt r√µ r√†ng v√† ho√†n ch·ªânh",
        ...
    ]
}}

CH·ªà TR·∫¢ V·ªÄ JSON VALID:
"""
            
            response = conversation.send_message(analysis_prompt)
            response_text = response.text.strip()
            
            # Clean and parse JSON
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            try:
                data = json.loads(response_text)
                questions = data.get("questions", [])
                context_summary = data.get("context_summary", "")
                
                # Filter out empty questions
                questions = [q.strip() for q in questions if q.strip()]
                
                logger.info(f"Extracted {len(questions)} questions and context summary")
                return questions, context_summary
                
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse Gemini JSON response: {e}")
                logger.error(f"Response text: {response_text}")
                
                # Fallback: try to extract basic info
                fallback_summary = f"Thread email v·ªõi {len(thread_emails)} tin nh·∫Øn"
                fallback_questions = []
                
                return fallback_questions, fallback_summary
            
        except Exception as e:
            logger.error(f"Error extracting questions with Gemini: {e}")
            return [], "L·ªói ph√¢n t√≠ch thread email"

    async def _ask_gemini(self, conversation: Any, prompt: str) -> str:
        try:
            response = conversation.send_message(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"Error asking Gemini: {e}")
            return f"L·ªói khi h·ªèi Gemini: {str(e)}"

    async def _generate_email_response_with_gemini(self, conversation: Any, thread_emails: List[Dict[str, Any]], summarized_results: List[Dict]) -> str:
        """
        Generate email response using Gemini with search results.
        
        Args:
            conversation: Gemini conversation object
            thread_emails: Original thread emails
            summarized_results: Results from Vietnamese Query Module
            
        Returns:
            Generated email response text
        """
        try:
            # Prepare unread student emails content
            student_questions = ""
            for email in thread_emails:
                if email['from'] and settings.GMAIL_EMAIL_ADDRESS not in email['from']:  # Student email
                    student_questions += f"T·ª´ sinh vi√™n: {email['content']}\n\n"
            
            email_prompt = f"""
D·ª±a tr√™n cu·ªôc h·ªôi tho·∫°i email v√† th√¥ng tin ƒë√£ t√¨m ƒë∆∞·ª£c, h√£y so·∫°n m·ªôt email ph·∫£n h·ªìi chuy√™n nghi·ªáp cho sinh vi√™n.

N·ªòI DUNG C√ÇU H·ªéI T·ª™ SINH VI√äN:
{student_questions}

TH√îNG TIN T√åM ƒê∆Ø·ª¢C:
"""
            for i, result in enumerate(summarized_results, 1):
                email_prompt += f"Nh√≥m th√¥ng tin {i}: {result['summary']}\n"
            
            email_prompt += f"""

Y√äU C·∫¶U SO·∫†N EMAIL:
- Vi·∫øt email ph·∫£n h·ªìi b·∫±ng ti·∫øng Vi·ªát chu·∫©n, chuy√™n nghi·ªáp
- ƒê·ªãnh d·∫°ng: vƒÉn b·∫£n thu·∫ßn (plain text), KH√îNG d√πng markdown
- C·∫•u tr√∫c: l·ªùi ch√†o, n·ªôi dung tr·∫£ l·ªùi t·ª´ng c√¢u h·ªèi, l·ªùi k·∫øt th√¢n thi·ªán
- Ghi r√µ ng√†y c·∫≠p nh·∫≠t th√¥ng tin khi c√≥ (∆∞u ti√™n th√¥ng tin m·ªõi nh·∫•t)
- Tr√≠ch d·∫´n ngu·ªìn th√¥ng tin ·ªü cu·ªëi email n·∫øu c·∫ßn (ch·ªâ v·ªõi th√¥ng tin t·ª´ t√†i li·ªáu ch√≠nh th·ª©c)
- Th√¥ng tin t·ª´ Q&A tr∆∞·ªõc ƒë√¢y c√≥ th·ªÉ s·ª≠ d·ª•ng tr·ª±c ti·∫øp m√† kh√¥ng c·∫ßn tr√≠ch d·∫´n ngu·ªìn
- Khi c√≥ nhi·ªÅu th√¥ng tin v·ªÅ c√πng ch·ªß ƒë·ªÅ, ∆∞u ti√™n v√† ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ ng√†y c·∫≠p nh·∫≠t g·∫ßn ƒë√¢y nh·∫•t
- N·∫øu thi·∫øu th√¥ng tin, h∆∞·ªõng d·∫´n sinh vi√™n li√™n h·ªá b·ªô ph·∫≠n ph√π h·ª£p
- K√Ω t√™n: "{settings.GMAIL_EMAIL_ADDRESS or 'Ph√≤ng C√¥ng t√°c Sinh vi√™n'}"

CH·ªà TR·∫¢ V·ªÄ N·ªòI DUNG EMAIL:
"""
            
            final_response = "C√≥ l·ªói x·∫£y ra khi t·∫°o ph·∫£n h·ªìi."
            if conversation and self.deepseek_client:
                try:
                    final_response = self.deepseek_client.send_message(
                        conversation=conversation,
                        message=email_prompt,
                        temperature=0.3,
                        max_tokens=8192,
                        error_default="C√≥ l·ªói x·∫£y ra khi t·∫°o ph·∫£n h·ªìi."
                    )
                except Exception as e:
                    logger.error(f"Error in conversation-based response generation: {e}")
                    final_response = "Xin l·ªói, c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i sau."
            else:
                logger.error("No conversation context available for response generation")
                final_response = "Kh√¥ng c√≥ context cu·ªôc h·ªôi tho·∫°i ƒë·ªÉ t·∫°o ph·∫£n h·ªìi."
            
            return final_response
            
        except Exception as e:
            logger.error(f"Error generating email response with Gemini: {e}")
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o email ph·∫£n h·ªìi. Vui l√≤ng li√™n h·ªá tr·ª±c ti·∫øp ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£.\n\nTr√¢n tr·ªçng,\n{settings.GMAIL_EMAIL_ADDRESS or 'Ph√≤ng C√¥ng t√°c Sinh vi√™n'}"

    async def create_draft_email(self, to: str, subject: str, body: str, thread_id: str = None) -> str:
        """
        Create a draft email in Gmail.
        
        Args:
            to: Recipient email address
            subject: Email subject
            body: Email body
            thread_id: Thread ID for linking draft to specific thread
            
        Returns:
            Draft ID if successful, None if failed
            
        Raises:
            Exception: If creating draft fails
        """
        try:
            message = MIMEMultipart()
            message['to'] = to
            message['subject'] = f"Re: {subject}"
            
            msg = MIMEText(body)
            message.attach(msg)
            
            encoded_message = base64.urlsafe_b64encode(
                message.as_bytes()).decode()
            
            draft_body = {'message': {'raw': encoded_message}}
            
            if thread_id:
                draft_body['message']['threadId'] = thread_id
                logger.info(f"Linking draft to thread: {thread_id}")
            else:
                logger.warning("No thread_id provided - draft will not be linked to any thread")
                
            draft = self.service.users().drafts().create(
                userId=self.user_id,
                body=draft_body
            ).execute()
            
            draft_id = draft['id']
            
            logger.info(f"Draft created with ID: {draft_id} {'(linked to thread: ' + thread_id + ')' if thread_id else '(no thread link)'}")
            return draft_id
            
        except Exception as e:
            logger.error(f"Error creating draft: {e}")
            raise
            
    
    async def process_text_with_vietnamese_query_module(self, text_content: str) -> str:
        """
        Process general text content with Vietnamese Query Module and generate comprehensive response
        
        Args:
            text_content: Input text content to process
            
        Returns:
            str: Comprehensive response with information and sources
        """
        try:
            if self.query_module is None:
                self._init_query_module()
            
            logger.info("Processing text with Vietnamese Query Module")
            results, conversation = self.query_module.process_text(text_content)
            
            if not results:
                logger.warning("No results from Vietnamese Query Module for text")
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn n·ªôi dung c·ªßa b·∫°n."
            
            # Validate conversation context
            if not conversation:
                logger.error("No conversation context available from query module")
                return "L·ªói: Kh√¥ng c√≥ context cu·ªôc h·ªôi tho·∫°i ƒë·ªÉ x·ª≠ l√Ω vƒÉn b·∫£n."
            
            logger.info(f"Successfully obtained conversation context and {len(results)} query results. Generating final response...")

            # Step 1: Filtering & Extraction
            retrieved_info = ""
            leaf_extraction_tasks = []

            for result in results:
                query = result.original_query
                if result.results:
                    for item in result.results:
                        content = item.get("content", "")
                        if content:
                            task = self._evaluate_and_extract_leaf_info(query, content)
                            leaf_extraction_tasks.append((query, item, task))
            
            if leaf_extraction_tasks:
                logger.info(f"Extracting 'Information Leaves' from {len(leaf_extraction_tasks)} retrieved chunks...")
                extracted_leaves = await asyncio.gather(*(task for _, _, task in leaf_extraction_tasks))
                logger.info("Extraction complete.")

                for (query, original_item, task), leaf_info in zip(leaf_extraction_tasks, extracted_leaves):
                    if leaf_info["is_relevant"]:
                        metadata = original_item.get("metadata", {})
                        file_created_at = metadata.get("file_created_at")
                        source = metadata.get("source")

                        retrieved_info += f"### Th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi: \"{query}\"\n\n"
                        retrieved_info += f"**Tr√≠ch xu·∫•t t·ª´ t√†i li·ªáu:**"
                        if source and not source.startswith("gmail_thread"):
                            retrieved_info += f" [Ngu·ªìn: {source}]"
                        if file_created_at:
                            retrieved_info += f" (C·∫≠p nh·∫≠t: {file_created_at})"
                        retrieved_info += f"\n---\n{leaf_info['leaf_content']}\n---\n\n"

            if not retrieved_info:
                retrieved_info = "H·ªá th·ªëng kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ n√†o sau khi ch·∫Øt l·ªçc."

            # Step 2: Synthesis & Response
            final_prompt = f"""
<instructions>
**VAI TR√í:**
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n gia, c√≥ nhi·ªám v·ª• t·ªïng h·ª£p th√¥ng tin t·ª´ c√°c ƒëo·∫°n tr√≠ch ƒë√£ ƒë∆∞·ª£c ch·∫Øt l·ªçc v√† so·∫°n m·ªôt c√¢u tr·∫£ l·ªùi cu·ªëi c√πng, m·∫°ch l·∫°c, ƒë·∫ßy ƒë·ªß cho ng∆∞·ªùi d√πng.

**B·ªêI C·∫¢NH:**
Ng∆∞·ªùi d√πng ƒë√£ ƒë∆∞a ra m·ªôt y√™u c·∫ßu/c√¢u h·ªèi. H·ªá th·ªëng ƒë√£ t√¨m ki·∫øm v√† sau ƒë√≥ ch·∫Øt l·ªçc ƒë·ªÉ l·∫•y ra nh·ªØng "l√° th√¥ng tin" (ƒëo·∫°n tr√≠ch) li√™n quan nh·∫•t d∆∞·ªõi ƒë√¢y.

**Y√äU C·∫¶U G·ªêC C·ª¶A NG∆Ø·ªúI D√ôNG:**
---
{text_content}
---

**C√ÅC L√Å TH√îNG TIN ƒê√É ƒê∆Ø·ª¢C CH·∫ÆT L·ªåC:**
---
{retrieved_info}
---

**NHI·ªÜM V·ª§:**
D·ª±a tr√™n **Y√äU C·∫¶U G·ªêC** v√† c√°c **L√Å TH√îNG TIN**, h√£y th·ª±c hi·ªán c√°c b∆∞·ªõc sau:
1.  **T·ªïng h·ª£p (Synthesize):** ƒê·ªçc v√† hi·ªÉu t·∫•t c·∫£ c√°c l√° th√¥ng tin. Li√™n k·∫øt ch√∫ng l·∫°i ƒë·ªÉ t·∫°o th√†nh m·ªôt b·ª©c tranh to√†n c·∫£nh.
2.  **L·ªçc v√† ∆Øu ti√™n (Filter & Prioritize):** N·∫øu c√≥ th√¥ng tin m√¢u thu·∫´n, h√£y ∆∞u ti√™n th√¥ng tin c√≥ ng√†y c·∫≠p nh·∫≠t m·ªõi nh·∫•t.
3.  **So·∫°n th·∫£o (Draft):** Vi·∫øt m·ªôt c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh, duy nh·∫•t.

**QUY T·∫ÆC SO·∫†N TH·∫¢O (B·∫ÆT BU·ªòC):**
*   **ƒê·ªãnh d·∫°ng:** Ch·ªâ s·ª≠ d·ª•ng vƒÉn b·∫£n thu·∫ßn (plain text). KH√îNG D√ôNG MARKDOWN.
*   **C·∫•u tr√∫c:** M·ªü ƒë·∫ßu ng·∫Øn g·ªçn, ƒëi th·∫≥ng v√†o n·ªôi dung ch√≠nh, tr·∫£ l·ªùi l·∫ßn l∆∞·ª£t t·ª´ng √Ω trong y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng, v√† k·∫øt lu·∫≠n.
*   **Tr√≠ch d·∫´n ng√†y:** Khi s·ª≠ d·ª•ng th√¥ng tin c√≥ ng√†y c·∫≠p nh·∫≠t, PH·∫¢I ghi r√µ trong c√¢u tr·∫£ l·ªùi (v√≠ d·ª•: "Theo quy ƒë·ªãnh c·∫≠p nh·∫≠t ng√†y 15/03/2024,...").
*   **Tr√≠ch d·∫´n ngu·ªìn:** N·∫øu th√¥ng tin c√≥ ngu·ªìn, h√£y ƒë√°nh s·ªë footnote trong c√¢u tr·∫£ l·ªùi (v√≠ d·ª•: `...n·ªôi dung [1].`) v√† li·ªát k√™ danh s√°ch ngu·ªìn ·ªü cu·ªëi c√πng d∆∞·ªõi ti√™u ƒë·ªÅ `NGU·ªíN THAM KH·∫¢O:`.
*   **Trung th·ª±c:** N·∫øu sau khi ch·∫Øt l·ªçc v·∫´n kh√¥ng c√≥ th√¥ng tin cho m·ªôt √Ω n√†o ƒë√≥, h√£y n√≥i r√µ "Hi·ªán t·∫°i h·ªá th·ªëng kh√¥ng t√¨m th·∫•y th√¥ng tin chi ti·∫øt v·ªÅ...".

Vi·∫øt c√¢u tr·∫£ l·ªùi cu·ªëi c√πng ngay d∆∞·ªõi ƒë√¢y.
</instructions>
"""

            final_response = "C√≥ l·ªói x·∫£y ra khi t·∫°o ph·∫£n h·ªìi."
            if conversation and self.deepseek_client:
                try:
                    final_response = self.deepseek_client.send_message(
                        conversation=conversation,
                        message=final_prompt,
                        temperature=0.3,
                        max_tokens=8192,
                        error_default="C√≥ l·ªói x·∫£y ra khi t·∫°o ph·∫£n h·ªìi."
                    )
                except Exception as e:
                    logger.error(f"Error in conversation-based response generation: {e}")
                    final_response = "Xin l·ªói, c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i sau."
            else:
                logger.error("No conversation context available for response generation")
                final_response = "Kh√¥ng c√≥ context cu·ªôc h·ªôi tho·∫°i ƒë·ªÉ t·∫°o ph·∫£n h·ªìi."
            
            return final_response
            
        except Exception as e:
            logger.warning(f"Error processing text with Vietnamese Query Module: {e}")
            return "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω vƒÉn b·∫£n. Vui l√≤ng th·ª≠ l·∫°i sau."

    async def _evaluate_and_extract_leaf_info(self, query: str, chunk_content: str) -> Dict[str, Any]:
        """
        C-RAG evaluation: Critique if chunk is relevant, then extract key information.
        Designed for concurrent execution without shared state.
        """
        if not query or not chunk_content or not self.deepseek_client:
            return {"is_relevant": False, "leaf_content": ""}

        try:
            system_message = "B·∫°n l√† m·ªôt AI chuy√™n gia ƒë√°nh gi√° v√† tr√≠ch xu·∫•t th√¥ng tin, ho·∫°t ƒë·ªông nh∆∞ m·ªôt b·ªô l·ªçc ch·∫•t l∆∞·ª£ng trong h·ªá th·ªëng RAG."
            
            user_message = f"""
<instructions>
**VAI TR√í:**
B·∫°n l√† m·ªôt AI chuy√™n gia ƒë√°nh gi√° v√† tr√≠ch xu·∫•t th√¥ng tin, ho·∫°t ƒë·ªông nh∆∞ m·ªôt b·ªô l·ªçc ch·∫•t l∆∞·ª£ng trong h·ªá th·ªëng RAG.

**NHI·ªÜM V·ª§:**
B·∫°n s·∫Ω th·ª±c hi·ªán m·ªôt quy tr√¨nh 2 b∆∞·ªõc:
1.  **B∆∞·ªõc 1: ƒê√°nh gi√° (Critique):** ƒê·ªçc k·ªπ c√¢u h·ªèi v√† vƒÉn b·∫£n. Quy·∫øt ƒë·ªãnh xem vƒÉn b·∫£n n√†y c√≥ ch·ª©a c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp ho·∫∑c th√¥ng tin c·ª±c k·ª≥ li√™n quan ƒë·∫øn c√¢u h·ªèi hay kh√¥ng.
2.  **B∆∞·ªõc 2: Tr√≠ch xu·∫•t (Extract):** N·∫øu v√† ch·ªâ n·∫øu vƒÉn b·∫£n ƒë∆∞·ª£c ƒë√°nh gi√° l√† c√≥ li√™n quan, h√£y tr√≠ch xu·∫•t nguy√™n vƒÉn nh·ªØng c√¢u ho·∫∑c c·ª•m c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi ƒë√≥.

**C√ÇU H·ªéI G·ªêC:**
---
{query}
---

**VƒÇN B·∫¢N C·∫¶N ƒê√ÅNH GI√Å V√Ä TR√çCH XU·∫§T:**
---
{chunk_content}
---

**ƒê·ªäNH D·∫†NG ƒê·∫¶U RA (B·∫ÆT BU·ªòC):**
Ch·ªâ tr·∫£ v·ªÅ m·ªôt ƒë·ªëi t∆∞·ª£ng JSON h·ª£p l·ªá v·ªõi c·∫•u tr√∫c sau:
```json
{{
  "is_relevant": <true n·∫øu vƒÉn b·∫£n c√≥ li√™n quan, ng∆∞·ª£c l·∫°i false>,
  "leaf_content": "<n·ªôi dung ƒë∆∞·ª£c tr√≠ch xu·∫•t n·∫øu is_relevant l√† true, ng∆∞·ª£c l·∫°i l√† chu·ªói r·ªóng>"
}}
```
</instructions>
"""
            
            response_text = await call_deepseek_async(
                deepseek_client=self.deepseek_client,
                system_message=system_message,
                user_message=user_message,
                temperature=0.0,
                max_tokens=4000,
                error_default='{"is_relevant": false, "leaf_content": ""}'
            )
            
            # Clean and parse JSON
            response_text = response_text.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            
            return json.loads(response_text.strip())
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Error during C-RAG evaluation for query '{query}': {e}")
            return {"is_relevant": False, "leaf_content": ""}

    def _search_multiple_collections(self, question: str) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Search in both main collection and EMAIL_QA collection using existing query module
        
        Args:
            question: The question to search for
            
        Returns:
            Tuple of (main_collection_results, email_qa_results)
        """
        if self.query_module is None:
            logger.warning("Query module not initialized")
            return [], []
        
        # Store original collection name
        original_collection = self.query_module.embedding_module.qdrant_manager.collection_name
        
        try:
            # Search in main collection (already configured)
            main_results = self.query_module.process_single_query(question)
            
            # Temporarily switch to EMAIL_QA collection
            self.query_module.embedding_module.qdrant_manager.collection_name = settings.EMAIL_QA_COLLECTION
            qa_results = self.query_module.process_single_query(question)
            
            logger.debug(f"Found {len(main_results)} results in main collection and {len(qa_results)} results in EMAIL_QA collection for question: {question[:50]}...")
            
            return main_results, qa_results
            
        except Exception as e:
            logger.error(f"Error searching multiple collections: {e}")
            return [], []
        finally:
            # Restore original collection name
            self.query_module.embedding_module.qdrant_manager.collection_name = original_collection

    async def run(self) -> None:
        
        if not self.service:
            self.authenticate()
            
        if not self.draft_monitor or not self.api_monitor:
            self._initialize_managers()
        
        if not self.background_worker:
            self._init_indexing_worker()
        
        if not self.cleanup_worker:
            self._init_cleanup_worker()
        
        logger.info("Starting Gmail monitoring with API polling")
        
        await self.api_monitor.start_monitoring()
        logger.info("Gmail API polling monitoring started")
                
async def start_gmail_monitoring(gmail_handler=None):
    if gmail_handler:
        logger.info("Using injected Gmail handler for monitoring")
        handler = gmail_handler
    else:
        logger.info("Creating new Gmail handler for monitoring")
        handler = GmailHandler()
    
    await handler.run()
