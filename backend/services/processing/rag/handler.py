import os
import base64
import json
import asyncio
import logging
import time
import uuid
import google.generativeai as genai
import functools

from datetime import datetime, time as datetime_time
from typing import Dict, Any, List, Optional, Tuple
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.utils import parsedate_to_datetime
from pathlib import Path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from backend.services.processing.rag.retrievers.qdrant_retriever import VietnameseQueryModule, create_query_module
from backend.services.processing.rag.embedders.text_embedder import VietnameseEmbeddingModule
from backend.common.config import settings
from backend.adapter.metadata import get_metadata_db

from backend.services.processing.rag.draft_monitor import EmailDraftMonitor

from backend.services.processing.rag.utils import (
    create_deepseek_client, DeepSeekAPIClient, 
    extract_image_attachments, extract_text_content, extract_all_attachments,
    call_deepseek_async
)

from backend.services.processing.rag.extractors.gemini.gemini_email_processor import GeminiEmailProcessor
from backend.services.processing.rag.gmail_api_monitor import create_gmail_api_monitor
from backend.services.processing.rag.gmail_indexing_worker import GmailIndexingWorker
from backend.services.processing.rag.gmail_cleanup_worker import GmailCleanupWorker

logger = logging.getLogger(__name__)

# Create log directory for query processing
QUERY_LOG_DIR = Path(__file__).resolve().parents[4] / "logs" / "query_processing"
QUERY_LOG_DIR.mkdir(parents=True, exist_ok=True)

class GmailHandler:
    """
    Gmail handler that processes emails with multimodal content using Gemini
    """
    
    def __init__(self, token_path: str = None):
        """
        Initialize Gmail handler with Gemini always enabled.
        
        Args:
            token_path: Path to the token JSON file  
        """
        self.token_path = token_path or settings.GMAIL_TOKEN_PATH
        self.service = None
        self.user_id = 'me'
        
        try:
            if settings.GOOGLE_API_KEY:
                genai.configure(api_key=settings.GOOGLE_API_KEY)
                logger.debug("Gemini API configured")
            
            self.gemini_processor = GeminiEmailProcessor()
            logger.debug("Gemini email processor initialized")
        except Exception as e:
            logger.error(f"Gemini initialization failed: {e}")
            raise Exception(f"Required Gemini processor failed to initialize: {e}")
        
        self.deepseek_api_key = settings.DEEPSEEK_API_KEY
        self.deepseek_api_url = settings.DEEPSEEK_API_URL
        self.deepseek_model = settings.DEEPSEEK_MODEL
        
        self.deepseek_client = create_deepseek_client(
            deepseek_api_key=self.deepseek_api_key,
            deepseek_api_url=self.deepseek_api_url,
            deepseek_model=self.deepseek_model
        )
        
        self.query_module = None
        
        self.metadata_db = get_metadata_db()
        
        self.draft_monitor = None
        self.api_monitor = None
        
        self.background_worker = None
        self.cleanup_worker = None
        
        if not self.deepseek_api_key:
            logger.warning("DEEPSEEK_API_KEY not set in settings")
    
    def _initialize_managers(self):
        """Initialize draft monitor and API monitor after authentication."""
        if not self.service:
            logger.error("Gmail service not authenticated, cannot initialize managers")
            return
        
        self.draft_monitor = EmailDraftMonitor(
            service=self.service,
            metadata_db=self.metadata_db,
            user_id=self.user_id
        )
        
        self.api_monitor = create_gmail_api_monitor(gmail_handler=self, poll_interval=settings.GMAIL_POLL_INTERVAL)
        
        logger.info("Draft monitor and Gmail API monitor initialized successfully")

    def _init_indexing_worker(self):
        """Initialize indexing worker (called separately after authentication)"""
        if not self.service:
            logger.error("Gmail service not authenticated, cannot initialize indexing worker")
            return False
        
        try:
            if self.query_module is None:
                self._init_query_module()
            
            embedding_module = None
            if hasattr(self.query_module, 'embedding_module'):
                embedding_module = self.query_module.embedding_module
                logger.info("Using shared embedding module from query module")
            
            self.background_worker = GmailIndexingWorker(
                gmail_service=self.service,
                user_id=self.user_id,
                gemini_processor=self.gemini_processor,
                embedding_module=embedding_module
            )
            self.background_worker.start()
            logger.info("Gmail indexing worker initialized and started")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize indexing worker: {e}")
            return False

    def _init_cleanup_worker(self):
        """Initialize cleanup worker for outdated threads"""
        try:
            # Initialize query module first to get embedding module
            if self.query_module is None:
                self._init_query_module()
            
            # Get embedding module from query module
            embedding_module = None
            if hasattr(self.query_module, 'embedding_module'):
                embedding_module = self.query_module.embedding_module
                logger.info("Using shared embedding module from query module")
            
            self.cleanup_worker = GmailCleanupWorker(embedding_module=embedding_module)
            self.cleanup_worker.start()
            logger.info("Gmail cleanup worker initialized and started")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize cleanup worker: {e}")
            return False

    def _init_query_module(self):
        """
        Initialize the Vietnamese Query Module if not already initialized
        """
        if self.query_module is not None:
            return
            
        try:
            # Try to get embedding module and memory manager from server modules
            memory_manager = None
            embedding_module = None
            try:
                # Import here to avoid circular imports
                from backend.services.processing.server import modules
                if hasattr(modules, 'cuda_memory_manager') and modules.cuda_memory_manager:
                    memory_manager = modules.cuda_memory_manager
                    logger.debug("Using shared CUDA Memory Manager from server")
                if hasattr(modules, 'embedding_module') and modules.embedding_module:
                    embedding_module = modules.embedding_module
                    logger.debug("Using shared Embedding Module from server")
            except (ImportError, AttributeError) as e:
                logger.warning(f"Could not import modules from server: {e}")
            
            # Create embedding module only if not available from server
            if not embedding_module:
                logger.info("Creating new VietnameseEmbeddingModule for Gmail handler")
                embedding_module = VietnameseEmbeddingModule(
                    qdrant_host=settings.QDRANT_HOST,
                    qdrant_port=settings.QDRANT_PORT,
                    collection_name=settings.QDRANT_COLLECTION_NAME,
                    dense_model_name=settings.DENSE_MODEL_NAME,
                    sparse_model_name=settings.SPARSE_MODEL_NAME,
                    reranker_model_name=settings.RERANKER_MODEL_NAME,
                    vector_size=settings.VECTOR_SIZE,
                    memory_manager=memory_manager 
                )
            
            self.query_module = create_query_module(
                embedding_module=embedding_module,
                deepseek_api_key=self.deepseek_api_key,
                memory_manager=memory_manager,  
                deepseek_model=self.deepseek_model,
                limit=5,
                candidates_limit=10,
                dense_weight=0.8,
                sparse_weight=0.2,
                normalization="min_max",
                candidates_multiplier=3
            )
            
            logger.debug(f"Vietnamese Query Module initialized with hybrid search and reranking")
            
        except Exception as e:
            logger.error(f"Error initializing Vietnamese Query Module: {e}")
            raise

    def authenticate(self) -> None:
        creds = None
        
        # Check if token file exists and load it directly
        if os.path.exists(self.token_path):
            try:
                # Load token data
                token_data = json.load(open(self.token_path))
                
                # Create credentials using the token data
                creds = Credentials.from_authorized_user_info(token_data)
                
                # If token is expired but has refresh token, refresh it
                if creds.expired and creds.refresh_token:
                    creds.refresh(Request())
                    # Save the refreshed token
                    with open(self.token_path, 'w') as token:
                        token.write(creds.to_json())
                    logger.info(f"Refreshed and saved authentication token to {self.token_path}")
            except Exception as e:
                logger.error(f"Error loading or refreshing token file: {e}")
                raise
        else:
            logger.error(f"Token file not found at {self.token_path}")
            raise FileNotFoundError(f"Token file not found: {self.token_path}")
                
        # Build Gmail service
        try:
            self.service = build('gmail', 'v1', credentials=creds)
            logger.info("Successfully authenticated with Gmail API")
            
            # Initialize managers after authentication
            self._initialize_managers()
        except Exception as e:
            logger.error(f"Error building Gmail service: {e}")
            raise
            
    async def process_unread_email(self) -> List[Dict[str, Any]]:
        """
        Process unread emails using new optimized logic with Gemini conversation.
        
        Returns:
            List of processed email thread information
            
        Raises:
            Exception: If processing emails fails
        """
        if not self.service:
            try:
                self.authenticate()
            except Exception as e:
                logger.error(f"Authentication failed: {e}")
                return []
                
        try:
            # Get unread messages
            results = self.service.users().messages().list(
                userId=self.user_id, 
                q="is:unread").execute()
                
            messages = results.get('messages', [])
            
            if not messages:
                logger.debug("No unread messages found")
                return []
            
            # Group messages by thread_id
            thread_groups = {}
            for message in messages:
                msg = self.service.users().messages().get(
                    userId=self.user_id, 
                    id=message['id']).execute()
                    
                thread_id = msg['threadId']
                if thread_id not in thread_groups:
                    thread_groups[thread_id] = []
                thread_groups[thread_id].append(msg)
            
            logger.info(f"Found {len(messages)} unread emails in {len(thread_groups)} threads")
            
            processed_results = []
            
            # Process each thread
            for thread_id, thread_messages in thread_groups.items():
                try:
                    result = await self._process_thread(thread_id, thread_messages)
                    if result:
                        processed_results.append(result)
                except Exception as e:
                    logger.error(f"Error processing thread {thread_id}: {e}")
                    continue
            
            return processed_results
            
        except HttpError as e:
            logger.error(f"Error fetching emails: {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            return []
            
    def _get_email_body(self, message: Dict) -> str:
        """
        Extract email body from Gmail message, including processing images with Gemini
        
        Args:
            message: Gmail message object
            
        Returns:
            Processed email body content
        """
        try:
            message_id = message.get('id')
            payload = message.get('payload', {})
            
            email_text = extract_text_content(payload)
            
            image_attachments = extract_image_attachments(self.service, self.user_id, payload, message_id)
            
            # Process attachments with Gemini if present
            if image_attachments:
                try:
                    logger.info(f"Processing {len(image_attachments)} images with Gemini")
                    return self.gemini_processor.process_email_with_attachments(
                        email_text=email_text, 
                        image_attachments=image_attachments,
                        pdf_attachments=[]
                    )
                except Exception as e:
                    logger.error(f"Gemini error: {e}")
            
            # Add image info if present
            if image_attachments:
                image_info = f"\n\n=== áº¢NH ÄÃNH KÃˆM ===\n"
                for i, img in enumerate(image_attachments, 1):
                    image_info += f"ğŸ“· áº¢nh {i}: {img.get('filename', f'image_{i}')}\n"
                return email_text + image_info
            
            return email_text
            
        except Exception as e:
            logger.error(f"Error extracting email body: {e}")
            return "[Lá»—i trÃ­ch xuáº¥t ná»™i dung email]"

    async def _process_thread(self, thread_id: str, thread_messages: List[Dict]) -> Optional[Dict[str, Any]]:
        try:
            logger.info(f"Processing thread {thread_id} with {len(thread_messages)} messages")
            
            existing_draft_id = self.draft_monitor.check_existing_draft(thread_id)
            if existing_draft_id:
                logger.info(f"Found existing draft {existing_draft_id}, deleting")
                self.draft_monitor.delete_draft(existing_draft_id)
            
            thread_info = self.metadata_db.get_gmail_thread_info(thread_id)
            last_processed_message_id = thread_info.get('last_processed_message_id') if thread_info else None
            
            all_thread_emails = await self._fetch_thread_emails_with_attachments(
                thread_id, last_processed_message_id
            )
            
            if not all_thread_emails:
                logger.warning(f"No emails to process for thread {thread_id}")
                return None
            
            conversation = await self._create_gemini_conversation_for_thread(all_thread_emails)
            if not conversation:
                logger.error(f"Failed to create Gemini conversation for thread {thread_id}")
                return None
            
            questions, context_summary = await self._extract_questions_with_gemini(conversation, all_thread_emails)
            
            if not questions:
                logger.info(f"No questions found in thread {thread_id}")
                return {"thread_id": thread_id, "status": "no_questions"}
            
            if self.query_module is None:
                self._init_query_module()
            
            summarized_results = []
            for i in range(0, len(questions), 2):
                group = questions[i:i+2]
                logger.debug(f"Processing question group {i//2 + 1}: {len(group)} questions")
                
                group_info = ""
                group_qa_info = ""  # Separate info for EMAIL_QA results
                group_queries = []
                
                for j, question in enumerate(group):
                    group_queries.append(question)
                    
                    # Search in both collections using optimized method
                    search_results, qa_results = self._search_multiple_collections(question)
                    
                    # Format main collection results
                    if search_results:
                        group_info += f"CÃ¢u há»i {j+1}: {question}\n"
                        for k, result_item in enumerate(search_results):
                            content = result_item.get("content", "") if isinstance(result_item, dict) else str(result_item)
                            metadata = result_item.get("metadata", {}) if isinstance(result_item, dict) else {}
                            file_created_at = metadata.get("file_created_at")
                            source = metadata.get("source")
                            
                            group_info += f"TÃ i liá»‡u {k+1}:"
                            if file_created_at:
                                group_info += f" (Cáº­p nháº­t: {file_created_at})"
                            if source and not source.startswith("gmail_thread"):
                                group_info += f" [Nguá»“n: {source}]"
                            group_info += f"\n{content}\n\n"
                    else:
                        group_info += f"CÃ¢u há»i {j+1}: {question}\nKhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan.\n\n"
                    
                    # Format EMAIL_QA collection results (without source citation requirement)
                    if qa_results:
                        group_qa_info += f"CÃ¢u há»i {j+1}: {question}\n"
                        for k, qa_item in enumerate(qa_results):
                            qa_content = qa_item.get("content", "") if isinstance(qa_item, dict) else str(qa_item)
                            qa_metadata = qa_item.get("metadata", {}) if isinstance(qa_item, dict) else {}
                            qa_file_created_at = qa_metadata.get("file_created_at")
                            
                            group_qa_info += f"Q&A {k+1}:"
                            if qa_file_created_at:
                                group_qa_info += f" (Cáº­p nháº­t: {qa_file_created_at})"
                            group_qa_info += f"\n{qa_content}\n\n"
                
                # Create combined summarization prompt
                summarization_prompt = f"""
                HÃ£y tÃ³m táº¯t láº¡i cÃ¡c ná»™i dung liÃªn quan Ä‘áº¿n cÃ¡c cÃ¢u há»i sau má»™t cÃ¡ch chÃ­nh xÃ¡c, Ä‘áº§y Ä‘á»§ thÃ´ng tin, sÃºc tÃ­ch:
                
                CÃ¡c cÃ¢u há»i: {', '.join(group_queries)}
                
                ThÃ´ng tin tá»« tÃ i liá»‡u chÃ­nh thá»©c:
                {group_info}
                
                ThÃ´ng tin tá»« Q&A trÆ°á»›c Ä‘Ã¢y:
                {group_qa_info}
                
                LÆ¯U Ã QUAN TRá»ŒNG:
                - Æ¯u tiÃªn thÃ´ng tin cÃ³ ngÃ y cáº­p nháº­t gáº§n Ä‘Ã¢y nháº¥t tá»« cáº£ hai nguá»“n (tÃ i liá»‡u chÃ­nh thá»©c vÃ  Q&A).
                - Náº¿u cÃ³ nhiá»u thÃ´ng tin vá» cÃ¹ng má»™t chá»§ Ä‘á» vá»›i cÃ¡c ngÃ y cáº­p nháº­t khÃ¡c nhau, chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« nguá»“n cÃ³ ngÃ y cáº­p nháº­t má»›i nháº¥t.
                - Khi cÃ³ thÃ´ng tin nguá»“n tá»« tÃ i liá»‡u chÃ­nh thá»©c, hÃ£y ghi rÃµ nguá»“n trong tÃ³m táº¯t Ä‘á»ƒ cÃ³ thá»ƒ trÃ­ch dáº«n sau nÃ y.
                - ThÃ´ng tin tá»« Q&A trÆ°á»›c Ä‘Ã¢y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ°ng khÃ´ng cáº§n trÃ­ch dáº«n nguá»“n cá»¥ thá»ƒ.
                - Äá»‘i vá»›i thÃ´ng tin khÃ´ng cÃ³ ngÃ y cáº­p nháº­t rÃµ rÃ ng, coi nhÆ° cÅ© hÆ¡n so vá»›i thÃ´ng tin cÃ³ ngÃ y cáº­p nháº­t.
                - Khi so sÃ¡nh thÃ´ng tin tá»« tÃ i liá»‡u chÃ­nh thá»©c vÃ  Q&A cÃ³ cÃ¹ng ngÃ y cáº­p nháº­t, Æ°u tiÃªn thÃ´ng tin tá»« tÃ i liá»‡u chÃ­nh thá»©c.
                - LuÃ´n ghi rÃµ ngÃ y cáº­p nháº­t thÃ´ng tin trong tÃ³m táº¯t khi cÃ³ (vÃ­ dá»¥: "Theo thÃ´ng tin cáº­p nháº­t ngÃ y 15/03/2024...").
                """
                
                try:
                    summary_response = await self._ask_gemini(conversation, summarization_prompt)
                    summarized_results.append({
                        "queries": group_queries,
                        "summary": summary_response
                    })
                except Exception as e:
                    logger.error(f"Error summarizing group {group_queries}: {e}")
                    summarized_results.append({
                        "queries": group_queries,
                        "summary": f"Lá»—i xá»­ lÃ½ thÃ´ng tin cho cÃ¡c cÃ¢u há»i: {', '.join(group_queries)}"
                    })
            
            email_response = await self._generate_email_response_with_gemini(
                conversation, all_thread_emails, summarized_results
            )
            
            conversation = None
            
            newest_email = thread_messages[-1]
            headers = {header['name']: header['value'] 
                      for header in newest_email['payload']['headers']}
            to_address = headers.get('From', '')
            subject = headers.get('Subject', '')
            
            draft_id = await self.create_draft_email(
                to=to_address,
                subject=subject,
                body=email_response,
                thread_id=thread_id
            )
            
            if draft_id:
                newest_message_id = newest_email['id']
                self.metadata_db.upsert_gmail_thread(
                    thread_id=thread_id,
                    context_summary=context_summary,
                    current_draft_id=draft_id,
                    last_processed_message_id=newest_message_id
                )
                
                marked_count = 0
                for msg in thread_messages:
                    try:
                        self.mark_as_read(msg['id'])
                        marked_count += 1
                    except Exception as mark_error:
                        logger.error(f"Failed to mark message {msg['id']} as read: {mark_error}")
                
                logger.info(f"Successfully processed thread {thread_id}, draft ID: {draft_id}, marked {marked_count}/{len(thread_messages)} messages as read")
                
                return {
                    "thread_id": thread_id,
                    "draft_id": draft_id,
                    "processed_messages": marked_count,
                    "context_summary": context_summary,
                    "status": "success"
                }
            else:
                logger.error(f"Failed to create draft for thread {thread_id}")
                return None
                
        except Exception as e:
            logger.error(f"Error in _process_thread for thread {thread_id}: {e}")
            return None

    
    def mark_as_read(self, message_id: str) -> None:
        """
        Mark an email as read.
        
        Args:
            message_id: Gmail message ID
            
        Raises:
            Exception: If marking as read fails
        """
        try:
            self.service.users().messages().modify(
                userId=self.user_id,
                id=message_id,
                body={'removeLabelIds': ['UNREAD']}
            ).execute()
            logger.info(f"Marked message {message_id} as read")
        except Exception as e:
            logger.error(f"Error marking message as read: {e}")
            raise
            

    async def _fetch_thread_emails_with_attachments(self, thread_id: str, last_processed_message_id: str = None) -> List[Dict[str, Any]]:
        try:
            thread_messages = self.service.users().threads().get(
                userId=self.user_id, 
                id=thread_id,
                format='full'
            ).execute()
            
            messages = thread_messages.get('messages', [])
            
            if not messages:
                return []
            
            filtered_messages = []
            if last_processed_message_id:
                found_last = False
                for message in messages:
                    if message['id'] == last_processed_message_id:
                        found_last = True
                        continue
                    if found_last:
                        filtered_messages.append(message)
                
                if not found_last:
                    logger.warning(f"Last processed message {last_processed_message_id} not found, processing all messages")
                    filtered_messages = messages
            else:
                filtered_messages = messages
            
            if not filtered_messages:
                logger.info(f"No new messages to process for thread {thread_id}")
                return []
            
            processed_emails = []
            for message in filtered_messages:
                try:
                    headers = {h['name']: h['value'] for h in message['payload']['headers']}
                    
                    # Extract text content
                    email_text = extract_text_content(message['payload'])
                    
                    # Extract all attachments (images and PDFs)
                    attachments = extract_all_attachments(
                        self.service, self.user_id, message['payload'], message['id']
                    )
                    
                    # Process with Gemini if attachments exist
                    if attachments:
                        image_attachments = [att for att in attachments if att.get('attachment_type') == 'image']
                        pdf_attachments = [att for att in attachments if att.get('attachment_type') == 'pdf']
                        
                        try:
                            processed_content = self.gemini_processor.process_email_with_attachments(
                                email_text=email_text, 
                                image_attachments=image_attachments,
                                pdf_attachments=pdf_attachments
                            )
                        except Exception as e:
                            logger.error(f"Gemini processing failed for message {message['id']}: {e}")
                            processed_content = email_text + f"\n--- Lá»—i xá»­ lÃ½ Ä‘Ã­nh kÃ¨m: {str(e)} ---"
                    else:
                        processed_content = email_text
                    
                    processed_emails.append({
                        'id': message['id'],
                        'from': headers.get('From', ''),
                        'to': headers.get('To', ''),
                        'subject': headers.get('Subject', ''),
                        'date': headers.get('Date', ''),
                        'content': processed_content,
                        'original_text': email_text,
                        'has_attachments': len(attachments) > 0,
                        'attachment_count': len(attachments)
                    })
                    
                except Exception as e:
                    logger.error(f"Error processing message {message['id']}: {e}")
                    continue
            
            logger.info(f"Processed {len(processed_emails)} emails from thread {thread_id}")
            return processed_emails
            
        except Exception as e:
            logger.error(f"Error fetching thread emails for {thread_id}: {e}")
            return []

    async def _create_gemini_conversation_for_thread(self, thread_emails: List[Dict[str, Any]]) -> Optional[Any]:
        try:
            import google.generativeai as genai
            
            gmail_address = settings.GMAIL_EMAIL_ADDRESS 
            system_message = f"""
Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn nghiá»‡p há»— trá»£ {gmail_address} trong viá»‡c phÃ¢n tÃ­ch vÃ  xá»­ lÃ½ email tá»« sinh viÃªn.

THÃ”NG TIN QUAN TRá»ŒNG:
- Báº¡n Ä‘ang há»— trá»£ tÃ i khoáº£n: {gmail_address}
- Vai trÃ²: Trá»£ lÃ½ phÃ²ng cÃ´ng tÃ¡c sinh viÃªn
- Nhiá»‡m vá»¥: PhÃ¢n tÃ­ch email, tÃ³m táº¯t ná»™i dung, trÃ­ch xuáº¥t cÃ¢u há»i vÃ  táº¡o pháº£n há»“i chuyÃªn nghiá»‡p

NGUYÃŠN Táº®C HOáº T Äá»˜NG:
1. PhÃ¢n tÃ­ch toÃ n bá»™ thread email Ä‘á»ƒ hiá»ƒu context Ä‘áº§y Ä‘á»§
2. TÃ³m táº¯t ná»™i dung má»™t cÃ¡ch chi tiáº¿t vÃ  chÃ­nh xÃ¡c
3. TrÃ­ch xuáº¥t cÃ¡c cÃ¢u há»i chÆ°a Ä‘Æ°á»£c giáº£i Ä‘Ã¡p tá»« sinh viÃªn
4. Táº¡o pháº£n há»“i chuyÃªn nghiá»‡p, thÃ¢n thiá»‡n nhÆ°ng trang trá»ng
5. Äáº£m báº£o thÃ´ng tin chÃ­nh xÃ¡c vÃ  cáº­p nháº­t

HÃ£y sáºµn sÃ ng phÃ¢n tÃ­ch thread email.
"""
            
            model = genai.GenerativeModel("gemini-2.0-flash")
            chat = model.start_chat(history=[])
            
            # Send system message
            response = chat.send_message(system_message)
            
            logger.info("Successfully created Gemini conversation for thread analysis")
            return chat
            
        except Exception as e:
            logger.error(f"Error creating Gemini conversation: {e}")
            return None

    async def _extract_questions_with_gemini(self, conversation: Any, thread_emails: List[Dict[str, Any]]) -> tuple[List[str], str]:
        """
        Extract questions and create context summary using Gemini.
        
        Args:
            conversation: Gemini conversation object
            thread_emails: List of thread email data
            
        Returns:
            Tuple of (questions_list, context_summary)
        """
        try:
            # Prepare thread content for analysis
            thread_content = ""
            for i, email in enumerate(thread_emails, 1):
                thread_content += f"""
=== EMAIL {i} ===
Tá»«: {email['from']}
Äáº¿n: {email['to']}
TiÃªu Ä‘á»: {email['subject']}
NgÃ y: {email['date']}
Ná»™i dung:
{email['content']}

"""
            
            analysis_prompt = f"""
HÃ£y phÃ¢n tÃ­ch thread email sau vÃ  thá»±c hiá»‡n 2 nhiá»‡m vá»¥:

1. TÃ“M Táº®T CONTEXT: Táº¡o tÃ³m táº¯t chi tiáº¿t, Ä‘áº§y Ä‘á»§ thÃ´ng tin vá» toÃ n bá»™ thread email (Ä‘á»‘i vá»›i ná»™i dung tá»« attachment thÃ¬ cáº§n ngáº¯n gá»n, sÃºc tÃ­ch nhÆ°ng váº«n Ä‘áº§y Ä‘á»§ thÃ´ng tin bá»• trá»£ cho email cá»§a ngÆ°á»i há»i)

2. TRÃCH XUáº¤T CÃ‚U Há»I: TÃ¬m táº¥t cáº£ cÃ¡c cÃ¢u há»i/yÃªu cáº§u thÃ´ng tin tá»« sinh viÃªn mÃ  chÆ°a Ä‘Æ°á»£c giáº£i Ä‘Ã¡p hoáº·c cáº§n thÃ´ng tin thÃªm

THREAD EMAIL:
{thread_content}

LÆ¯U Ã QUAN TRá»ŒNG:
- Chá»‰ trÃ­ch xuáº¥t cÃ¢u há»i tá»« email sinh viÃªn (khÃ´ng pháº£i tá»« email pháº£n há»“i cá»§a phÃ²ng cÃ´ng tÃ¡c sinh viÃªn)
- CÃ¢u há»i pháº£i rÃµ rÃ ng vÃ  cáº§n thÃ´ng tin cá»¥ thá»ƒ
- Bá» qua cÃ¡c lá»i chÃ o há»i, cáº£m Æ¡n Ä‘Æ¡n thuáº§n
- Má»—i cÃ¢u há»i pháº£i hoÃ n chá»‰nh vÃ  cÃ³ thá»ƒ tÃ¬m kiáº¿m Ä‘Æ°á»£c

Tráº£ vá» JSON vá»›i format:
{{
    "context_summary": "TÃ³m táº¯t chi tiáº¿t toÃ n bá»™ thread email...",
    "questions": [
        "CÃ¢u há»i 1 Ä‘Æ°á»£c viáº¿t rÃµ rÃ ng vÃ  hoÃ n chá»‰nh",
        "CÃ¢u há»i 2 Ä‘Æ°á»£c viáº¿t rÃµ rÃ ng vÃ  hoÃ n chá»‰nh",
        ...
    ]
}}

CHá»ˆ TRáº¢ Vá»€ JSON VALID:
"""
            
            response = conversation.send_message(analysis_prompt)
            response_text = response.text.strip()
            
            # Clean and parse JSON
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            try:
                data = json.loads(response_text)
                questions = data.get("questions", [])
                context_summary = data.get("context_summary", "")
                
                # Filter out empty questions
                questions = [q.strip() for q in questions if q.strip()]
                
                logger.info(f"Extracted {len(questions)} questions and context summary")
                return questions, context_summary
                
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse Gemini JSON response: {e}")
                logger.error(f"Response text: {response_text}")
                
                # Fallback: try to extract basic info
                fallback_summary = f"Thread email vá»›i {len(thread_emails)} tin nháº¯n"
                fallback_questions = []
                
                return fallback_questions, fallback_summary
            
        except Exception as e:
            logger.error(f"Error extracting questions with Gemini: {e}")
            return [], "Lá»—i phÃ¢n tÃ­ch thread email"

    async def _ask_gemini(self, conversation: Any, prompt: str) -> str:
        try:
            response = conversation.send_message(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"Error asking Gemini: {e}")
            return f"Lá»—i khi há»i Gemini: {str(e)}"

    async def _generate_email_response_with_gemini(self, conversation: Any, thread_emails: List[Dict[str, Any]], summarized_results: List[Dict]) -> str:
        """
        Generate email response using Gemini with search results.
        
        Args:
            conversation: Gemini conversation object
            thread_emails: Original thread emails
            summarized_results: Results from Vietnamese Query Module
            
        Returns:
            Generated email response text
        """
        try:
            # Prepare unread student emails content
            student_questions = ""
            for email in thread_emails:
                if email['from'] and settings.GMAIL_EMAIL_ADDRESS not in email['from']:  # Student email
                    student_questions += f"Tá»« sinh viÃªn: {email['content']}\n\n"
            
            email_prompt = f"""
Dá»±a trÃªn cuá»™c há»™i thoáº¡i email vÃ  thÃ´ng tin Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c, hÃ£y soáº¡n má»™t email pháº£n há»“i chuyÃªn nghiá»‡p cho sinh viÃªn.

Ná»˜I DUNG CÃ‚U Há»I Tá»ª SINH VIÃŠN:
{student_questions}

THÃ”NG TIN TÃŒM ÄÆ¯á»¢C:
"""
            for i, result in enumerate(summarized_results, 1):
                email_prompt += f"NhÃ³m thÃ´ng tin {i}: {result['summary']}\n"
            
            email_prompt += f"""

YÃŠU Cáº¦U SOáº N EMAIL:
- Viáº¿t email pháº£n há»“i báº±ng tiáº¿ng Viá»‡t chuáº©n, chuyÃªn nghiá»‡p
- Äá»‹nh dáº¡ng: vÄƒn báº£n thuáº§n (plain text), KHÃ”NG dÃ¹ng markdown
- Cáº¥u trÃºc: lá»i chÃ o, ná»™i dung tráº£ lá»i tá»«ng cÃ¢u há»i, lá»i káº¿t thÃ¢n thiá»‡n
- Ghi rÃµ ngÃ y cáº­p nháº­t thÃ´ng tin khi cÃ³ (Æ°u tiÃªn thÃ´ng tin má»›i nháº¥t)
- TrÃ­ch dáº«n nguá»“n thÃ´ng tin á»Ÿ cuá»‘i email náº¿u cáº§n (chá»‰ vá»›i thÃ´ng tin tá»« tÃ i liá»‡u chÃ­nh thá»©c)
- ThÃ´ng tin tá»« Q&A trÆ°á»›c Ä‘Ã¢y cÃ³ thá»ƒ sá»­ dá»¥ng trá»±c tiáº¿p mÃ  khÃ´ng cáº§n trÃ­ch dáº«n nguá»“n
- Khi cÃ³ nhiá»u thÃ´ng tin vá» cÃ¹ng chá»§ Ä‘á», Æ°u tiÃªn vÃ  chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ ngÃ y cáº­p nháº­t gáº§n Ä‘Ã¢y nháº¥t
- Náº¿u thiáº¿u thÃ´ng tin, hÆ°á»›ng dáº«n sinh viÃªn liÃªn há»‡ bá»™ pháº­n phÃ¹ há»£p
- KÃ½ tÃªn: "{settings.GMAIL_EMAIL_ADDRESS or 'PhÃ²ng CÃ´ng tÃ¡c Sinh viÃªn'}"

CHá»ˆ TRáº¢ Vá»€ Ná»˜I DUNG EMAIL:
"""
            
            final_response = "CÃ³ lá»—i xáº£y ra khi táº¡o pháº£n há»“i."
            if conversation and self.deepseek_client:
                try:
                    final_response = self.deepseek_client.send_message(
                        conversation=conversation,
                        message=email_prompt,
                        temperature=0.3,
                        max_tokens=8192,
                        error_default="CÃ³ lá»—i xáº£y ra khi táº¡o pháº£n há»“i."
                    )
                except Exception as e:
                    logger.error(f"Error in conversation-based response generation: {e}")
                    final_response = "Xin lá»—i, cÃ³ lá»—i xáº£y ra trong quÃ¡ trÃ¬nh táº¡o pháº£n há»“i. Vui lÃ²ng thá»­ láº¡i sau."
            else:
                logger.error("No conversation context available for response generation")
                final_response = "KhÃ´ng cÃ³ context cuá»™c há»™i thoáº¡i Ä‘á»ƒ táº¡o pháº£n há»“i."
            
            return final_response
            
        except Exception as e:
            logger.error(f"Error generating email response with Gemini: {e}")
            return f"Xin lá»—i, cÃ³ lá»—i xáº£y ra khi táº¡o email pháº£n há»“i. Vui lÃ²ng liÃªn há»‡ trá»±c tiáº¿p Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£.\n\nTrÃ¢n trá»ng,\n{settings.GMAIL_EMAIL_ADDRESS or 'PhÃ²ng CÃ´ng tÃ¡c Sinh viÃªn'}"

    async def create_draft_email(self, to: str, subject: str, body: str, thread_id: str = None) -> str:
        """
        Create a draft email in Gmail.
        
        Args:
            to: Recipient email address
            subject: Email subject
            body: Email body
            thread_id: Thread ID for linking draft to specific thread
            
        Returns:
            Draft ID if successful, None if failed
            
        Raises:
            Exception: If creating draft fails
        """
        try:
            message = MIMEMultipart()
            message['to'] = to
            message['subject'] = f"Re: {subject}"
            
            msg = MIMEText(body)
            message.attach(msg)
            
            encoded_message = base64.urlsafe_b64encode(
                message.as_bytes()).decode()
            
            draft_body = {'message': {'raw': encoded_message}}
            
            if thread_id:
                draft_body['message']['threadId'] = thread_id
                logger.info(f"Linking draft to thread: {thread_id}")
            else:
                logger.warning("No thread_id provided - draft will not be linked to any thread")
                
            draft = self.service.users().drafts().create(
                userId=self.user_id,
                body=draft_body
            ).execute()
            
            draft_id = draft['id']
            
            logger.info(f"Draft created with ID: {draft_id} {'(linked to thread: ' + thread_id + ')' if thread_id else '(no thread link)'}")
            return draft_id
            
        except Exception as e:
            logger.error(f"Error creating draft: {e}")
            raise
            
    
    def _save_query_processing_log(self, text_content: str, results: List, leaf_extraction_data: List, final_response: str, session_id: str) -> None:
        try:
            leaf_content_map = {}
            for query, original_item, leaf_info in leaf_extraction_data:
                if leaf_info.get("is_relevant", False):
                    if query not in leaf_content_map:
                        leaf_content_map[query] = []
                    leaf_content_map[query].append(leaf_info.get("leaf_content", ""))
            
            for i, result in enumerate(results):
                query = result.original_query
                safe_query_name = "".join(c for c in query if c.isalnum() or c in (' ', '-', '_')).rstrip()[:50]
                query_folder_name = f"query_{i+1:02d}_{safe_query_name}"
                query_folder = QUERY_LOG_DIR / session_id / query_folder_name
                query_folder.mkdir(parents=True, exist_ok=True)
                
                query_results_data = {
                    "original_text": text_content,
                    "query": query,
                    "results": [{"content": item.get("content", ""), "score": item.get("score", 0.0)} for item in result.results]
                }
                
                with open(query_folder / "01_search_results.json", 'w', encoding='utf-8') as f:
                    json.dump(query_results_data, f, ensure_ascii=False, indent=2)
                
                leaf_data = {"query": query, "leaf_contents": leaf_content_map.get(query, [])}
                
                with open(query_folder / "02_leaf_content.json", 'w', encoding='utf-8') as f:
                    json.dump(leaf_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Saved {len(results)} query folders in session: {session_id}")
            
        except Exception as e:
            logger.error(f"Error saving query logs: {e}")

    async def process_text_with_vietnamese_query_module(self, text_content: str) -> str:
        """
        Process general text content with Vietnamese Query Module and generate comprehensive response
        
        Args:
            text_content: Input text content to process
            
        Returns:
            str: Comprehensive response with information and sources
        """
        # Generate unique session ID for this processing session
        session_id = f"query_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"
        
        try:
            if self.query_module is None:
                self._init_query_module()
            
            logger.info(f"Processing text with Vietnamese Query Module - Session: {session_id}")
            results, conversation = self.query_module.process_text(text_content)
            
            if not results:
                logger.warning("No results from Vietnamese Query Module for text")
                return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘áº¿n ná»™i dung cá»§a báº¡n."
            
            # Validate conversation context
            if not conversation:
                logger.error("No conversation context available from query module")
                return "Lá»—i: KhÃ´ng cÃ³ context cuá»™c há»™i thoáº¡i Ä‘á»ƒ xá»­ lÃ½ vÄƒn báº£n."
            
            logger.info(f"Successfully obtained conversation context and {len(results)} query results. Generating final response...")

            # Step 1: Filtering & Extraction
            retrieved_info = ""
            leaf_extraction_tasks = []

            for result in results:
                query = result.original_query
                if result.results:
                    for item in result.results:
                        content = item.get("content", "")
                        if content:
                            task = self._evaluate_and_extract_leaf_info(query, content)
                            leaf_extraction_tasks.append((query, item, task))
            
            leaf_extraction_data = []
            if leaf_extraction_tasks:
                logger.info(f"Extracting 'Core Snippets' from {len(leaf_extraction_tasks)} retrieved chunks...")
                extracted_leaves = await asyncio.gather(*(task for _, _, task in leaf_extraction_tasks))
                logger.info("Extraction complete.")

                for (query, original_item, task), leaf_info in zip(leaf_extraction_tasks, extracted_leaves):
                    # Store for logging
                    leaf_extraction_data.append((query, original_item, leaf_info))
                    
                    if leaf_info["is_relevant"]:
                        metadata = original_item.get("metadata", {})
                        file_created_at = metadata.get("file_created_at")
                        source = metadata.get("source")

                        retrieved_info += f"### ThÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i: \"{query}\"\n\n"
                        retrieved_info += f"**TrÃ­ch xuáº¥t tá»« tÃ i liá»‡u:**"
                        if source and not source.startswith("gmail_thread"):
                            retrieved_info += f" [Nguá»“n: {source}]"
                        if file_created_at:
                            retrieved_info += f" (Cáº­p nháº­t: {file_created_at})"
                        retrieved_info += f"\n---\n{leaf_info['leaf_content']}\n---\n\n"

            if not retrieved_info:
                retrieved_info = "Há»‡ thá»‘ng khÃ´ng tÃ¬m tháº¥y thÃ´ng tin cá»¥ thá»ƒ nÃ o sau khi cháº¯t lá»c."

            # Step 2: Synthesis & Response
            final_prompt = f"""
<instructions>
**VAI TRÃ’:**
Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn gia, cÃ³ nhiá»‡m vá»¥ tá»•ng há»£p thÃ´ng tin tá»« cÃ¡c Ä‘oáº¡n trÃ­ch Ä‘Ã£ Ä‘Æ°á»£c cháº¯t lá»c vÃ  soáº¡n má»™t cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng, máº¡ch láº¡c, Ä‘áº§y Ä‘á»§ cho ngÆ°á»i dÃ¹ng.

**Bá»I Cáº¢NH:**
NgÆ°á»i dÃ¹ng Ä‘Ã£ Ä‘Æ°a ra má»™t yÃªu cáº§u/cÃ¢u há»i. Há»‡ thá»‘ng Ä‘Ã£ tÃ¬m kiáº¿m vÃ  sau Ä‘Ã³ cháº¯t lá»c Ä‘á»ƒ láº¥y ra nhá»¯ng **Ä‘oáº¡n trÃ­ch cá»‘t lÃµi** liÃªn quan nháº¥t dÆ°á»›i Ä‘Ã¢y.

**YÃŠU Cáº¦U Gá»C Cá»¦A NGÆ¯á»œI DÃ™NG:**
---
{text_content}
---

**CÃC ÄOáº N TRÃCH Cá»T LÃ•I ÄÃƒ ÄÆ¯á»¢C CHáº®T Lá»ŒC:**
---
{retrieved_info}
---

**NHIá»†M Vá»¤:**
Dá»±a trÃªn **YÃŠU Cáº¦U Gá»C** vÃ  cÃ¡c **ÄOáº N TRÃCH Cá»T LÃ•I**, hÃ£y thá»±c hiá»‡n cÃ¡c bÆ°á»›c sau:
1.  **Tá»•ng há»£p (Synthesize):** Äá»c vÃ  hiá»ƒu táº¥t cáº£ cÃ¡c Ä‘oáº¡n trÃ­ch cá»‘t lÃµi. LiÃªn káº¿t chÃºng láº¡i Ä‘á»ƒ táº¡o thÃ nh má»™t bá»©c tranh toÃ n cáº£nh.
2.  **Lá»c vÃ  Æ¯u tiÃªn (Filter & Prioritize):** Náº¿u cÃ³ thÃ´ng tin mÃ¢u thuáº«n, hÃ£y Æ°u tiÃªn thÃ´ng tin cÃ³ ngÃ y cáº­p nháº­t má»›i nháº¥t.
3.  **Soáº¡n tháº£o (Draft):** Viáº¿t má»™t cÃ¢u tráº£ lá»i hoÃ n chá»‰nh, duy nháº¥t.

**QUY Táº®C SOáº N THáº¢O (Báº®T BUá»˜C):**
*   **Äá»‹nh dáº¡ng:** Chá»‰ sá»­ dá»¥ng vÄƒn báº£n thuáº§n (plain text). KHÃ”NG DÃ™NG MARKDOWN.
*   **Cáº¥u trÃºc:** Má»Ÿ Ä‘áº§u ngáº¯n gá»n, Ä‘i tháº³ng vÃ o ná»™i dung chÃ­nh, tráº£ lá»i láº§n lÆ°á»£t tá»«ng Ã½ trong yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng, vÃ  káº¿t luáº­n.
*   **TrÃ­ch dáº«n ngÃ y:** Khi sá»­ dá»¥ng thÃ´ng tin cÃ³ ngÃ y cáº­p nháº­t, PHáº¢I ghi rÃµ trong cÃ¢u tráº£ lá»i (vÃ­ dá»¥: "Theo quy Ä‘á»‹nh cáº­p nháº­t ngÃ y 15/03/2024,...").
*   **TrÃ­ch dáº«n nguá»“n:** Náº¿u thÃ´ng tin cÃ³ nguá»“n, hÃ£y Ä‘Ã¡nh sá»‘ footnote trong cÃ¢u tráº£ lá»i (vÃ­ dá»¥: `...ná»™i dung [1].`) vÃ  liá»‡t kÃª danh sÃ¡ch nguá»“n á»Ÿ cuá»‘i cÃ¹ng dÆ°á»›i tiÃªu Ä‘á» `NGUá»’N THAM KHáº¢O:`. Náº¿u khÃ´ng cÃ³ thÃ´ng tin nÃ o Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« nguá»“n, **TUYá»†T Äá»I KHÃ”NG** hiá»ƒn thá»‹ má»¥c "NGUá»’N THAM KHáº¢O".
*   **Trung thá»±c:** Náº¿u sau khi cháº¯t lá»c váº«n khÃ´ng cÃ³ thÃ´ng tin cho má»™t Ã½ nÃ o Ä‘Ã³, hÃ£y nÃ³i rÃµ "Hiá»‡n táº¡i há»‡ thá»‘ng khÃ´ng tÃ¬m tháº¥y thÃ´ng tin chi tiáº¿t vá»...".

Viáº¿t cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng ngay dÆ°á»›i Ä‘Ã¢y.
</instructions>
"""

            final_response = "CÃ³ lá»—i xáº£y ra khi táº¡o pháº£n há»“i."
            if conversation and self.deepseek_client:
                try:
                    final_response = self.deepseek_client.send_message(
                        conversation=conversation,
                        message=final_prompt,
                        temperature=0.3,
                        max_tokens=8192,
                        error_default="CÃ³ lá»—i xáº£y ra khi táº¡o pháº£n há»“i."
                    )
                except Exception as e:
                    logger.error(f"Error in conversation-based response generation: {e}")
                    final_response = "Xin lá»—i, cÃ³ lá»—i xáº£y ra trong quÃ¡ trÃ¬nh táº¡o pháº£n há»“i. Vui lÃ²ng thá»­ láº¡i sau."
            else:
                logger.error("No conversation context available for response generation")
                final_response = "KhÃ´ng cÃ³ context cuá»™c há»™i thoáº¡i Ä‘á»ƒ táº¡o pháº£n há»“i."
            
            # Save logs
            self._save_query_processing_log(text_content, results, leaf_extraction_data, final_response, session_id)
            
            return final_response
            
        except Exception as e:
            logger.warning(f"Error processing text with Vietnamese Query Module: {e}")
            return "Xin lá»—i, cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ vÄƒn báº£n. Vui lÃ²ng thá»­ láº¡i sau."

    async def _evaluate_and_extract_leaf_info(self, query: str, chunk_content: str) -> Dict[str, Any]:
        """
        C-RAG evaluation: Critique if chunk is relevant, then extract key information.
        Designed for concurrent execution without shared state.
        """
        if not query or not chunk_content or not self.deepseek_client:
            return {"is_relevant": False, "leaf_content": ""}

        try:
            system_message = "Báº¡n lÃ  má»™t AI chuyÃªn gia Ä‘Ã¡nh giÃ¡ vÃ  trÃ­ch xuáº¥t thÃ´ng tin, hoáº¡t Ä‘á»™ng nhÆ° má»™t bá»™ lá»c cháº¥t lÆ°á»£ng trong há»‡ thá»‘ng RAG."
            
            user_message = f"""
<instructions>
**VAI TRÃ’:**
Báº¡n lÃ  má»™t AI chuyÃªn gia Ä‘Ã¡nh giÃ¡ vÃ  trÃ­ch xuáº¥t thÃ´ng tin, hoáº¡t Ä‘á»™ng nhÆ° má»™t bá»™ lá»c cháº¥t lÆ°á»£ng trong há»‡ thá»‘ng RAG.

**NHIá»†M Vá»¤:**
Báº¡n sáº½ thá»±c hiá»‡n má»™t quy trÃ¬nh 2 bÆ°á»›c:
1.  **BÆ°á»›c 1: ÄÃ¡nh giÃ¡ (Critique):** Äá»c ká»¹ cÃ¢u há»i vÃ  vÄƒn báº£n. Quyáº¿t Ä‘á»‹nh xem vÄƒn báº£n nÃ y cÃ³ chá»©a cÃ¢u tráº£ lá»i trá»±c tiáº¿p hoáº·c thÃ´ng tin cá»±c ká»³ liÃªn quan Ä‘áº¿n cÃ¢u há»i hay khÃ´ng.
2.  **BÆ°á»›c 2: TrÃ­ch xuáº¥t (Extract):** Náº¿u vÃ  chá»‰ náº¿u vÄƒn báº£n Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ lÃ  cÃ³ liÃªn quan, hÃ£y trÃ­ch xuáº¥t nguyÃªn vÄƒn nhá»¯ng cÃ¢u hoáº·c cá»¥m cÃ¢u tráº£ lá»i cho cÃ¢u há»i Ä‘Ã³ thÃ nh má»™t **Ä‘oáº¡n trÃ­ch cá»‘t lÃµi**.

**CÃ‚U Há»I Gá»C:**
---
{query}
---

**VÄ‚N Báº¢N Cáº¦N ÄÃNH GIÃ VÃ€ TRÃCH XUáº¤T:**
---
{chunk_content}
---

**Äá»ŠNH Dáº NG Äáº¦U RA (Báº®T BUá»˜C):**
Chá»‰ tráº£ vá» má»™t Ä‘á»‘i tÆ°á»£ng JSON há»£p lá»‡ vá»›i cáº¥u trÃºc sau:
```json
{{
  "is_relevant": <true náº¿u vÄƒn báº£n cÃ³ liÃªn quan, ngÆ°á»£c láº¡i false>,
  "leaf_content": "<ná»™i dung Ä‘Æ°á»£c trÃ­ch xuáº¥t náº¿u is_relevant lÃ  true, ngÆ°á»£c láº¡i lÃ  chuá»—i rá»—ng>"
}}
```
</instructions>
"""
            
            response_text = await call_deepseek_async(
                deepseek_client=self.deepseek_client,
                system_message=system_message,
                user_message=user_message,
                temperature=0.0,
                max_tokens=4000,
                error_default='{"is_relevant": false, "leaf_content": ""}'
            )
            
            # Clean and parse JSON
            response_text = response_text.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            
            return json.loads(response_text.strip())
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Error during C-RAG evaluation for query '{query}': {e}")
            return {"is_relevant": False, "leaf_content": ""}

    def _search_multiple_collections(self, question: str) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Search in both main collection and EMAIL_QA collection using existing query module
        
        Args:
            question: The question to search for
            
        Returns:
            Tuple of (main_collection_results, email_qa_results)
        """
        if self.query_module is None:
            logger.warning("Query module not initialized")
            return [], []
        
        # Store original collection name
        original_collection = self.query_module.embedding_module.qdrant_manager.collection_name
        
        try:
            # Search in main collection (already configured)
            main_results = self.query_module.process_single_query(question)
            
            # Temporarily switch to EMAIL_QA collection
            self.query_module.embedding_module.qdrant_manager.collection_name = settings.EMAIL_QA_COLLECTION
            qa_results = self.query_module.process_single_query(question)
            
            logger.debug(f"Found {len(main_results)} results in main collection and {len(qa_results)} results in EMAIL_QA collection for question: {question[:50]}...")
            
            return main_results, qa_results
            
        except Exception as e:
            logger.error(f"Error searching multiple collections: {e}")
            return [], []
        finally:
            # Restore original collection name
            self.query_module.embedding_module.qdrant_manager.collection_name = original_collection

    async def run(self) -> None:
        
        if not self.service:
            self.authenticate()
            
        if not self.draft_monitor or not self.api_monitor:
            self._initialize_managers()
        
        if not self.background_worker:
            self._init_indexing_worker()
        
        if not self.cleanup_worker:
            self._init_cleanup_worker()
        
        logger.info("Starting Gmail monitoring with API polling")
        
        await self.api_monitor.start_monitoring()
        logger.info("Gmail API polling monitoring started")
                
async def start_gmail_monitoring(gmail_handler=None):
    if gmail_handler:
        logger.info("Using injected Gmail handler for monitoring")
        handler = gmail_handler
    else:
        logger.info("Creating new Gmail handler for monitoring")
        handler = GmailHandler()
    
    await handler.run()
